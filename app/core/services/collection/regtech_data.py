"""REGTECH ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
ì‹¤ì œ REGTECH í¬í„¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
"""

import logging
import requests
from datetime import datetime
from datetime import date
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class REGTECHDataCollector:
    """REGTECH ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.base_url = "https://regtech.fsec.or.kr"

    def collect_real_regtech_data(self, session, regtech_id: str) -> dict:
        """ì‹¤ì œ REGTECH í¬í„¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ - ë™ì  URL ë°œê²¬ í¬í•¨"""
        try:
            logger.info(f"ğŸ” REGTECH ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {regtech_id}")

            # ì„¸ì…˜ ìœ íš¨ì„± ê²€ì¦
            if not session or not session.cookies:
                logger.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜")
                return {
                    "success": False,
                    "error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜ì…ë‹ˆë‹¤",
                    "collected_count": 0,
                }

            # ì„¸ì…˜ ì¿ í‚¤ í™•ì¸ ë¡œê·¸
            cookies = dict(session.cookies)
            logger.info(f"ğŸª ì‚¬ìš© ì¤‘ì¸ ì„¸ì…˜ ì¿ í‚¤: {list(cookies.keys())}")

            # 1ë‹¨ê³„: ë©”ì¸ í¬í„¸ í˜ì´ì§€ì—ì„œ ë°ì´í„° URL ë°œê²¬ ì‹œë„
            logger.info("ğŸ” REGTECH í¬í„¸ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            data_urls = self._discover_data_urls(session)

            # 2ë‹¨ê³„: ë°œê²¬ëœ URLë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
            for url_info in data_urls:
                url = url_info["url"]
                url_type = url_info["type"]

                logger.info(f"ğŸ“Š ë°ì´í„° URL ì‹œë„ ({url_type}): {url}")

                try:
                    response = session.get(url, timeout=30)

                    if response.status_code == 302:
                        logger.warning(f"âš ï¸ ì„¸ì…˜ ë§Œë£Œë¡œ ì¸í•œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€ ({url})")
                        return {
                            "success": False,
                            "error": "ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                            "collected_count": 0,
                            "session_expired": True,
                        }

                    if response.status_code == 200:
                        logger.info(f"âœ… ë°ì´í„° í˜ì´ì§€ ì ‘ê·¼ ì„±ê³µ: {url}")

                        # HTML íŒŒì‹± ë° IP ì¶”ì¶œ
                        collected_ips = self._parse_regtech_data(response.text)

                        if collected_ips:
                            logger.info(
                                f"âœ… REGTECHì—ì„œ {len(collected_ips)}ê°œ IP ìˆ˜ì§‘ ì™„ë£Œ"
                            )
                            return {
                                "success": True,
                                "message": f"REGTECH ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_ips)}ê°œ IP",
                                "data": collected_ips,
                                "collected_count": len(collected_ips),
                                "source": "regtech_real",
                                "session_reused": True,
                                "data_url": url,
                            }
                        else:
                            logger.info(f"âš ï¸ ë°ì´í„° ì—†ìŒ: {url}")

                    else:
                        logger.warning(
                            f"âš ï¸ ë°ì´í„° í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code} ({url})"
                        )

                except Exception as url_error:
                    logger.warning(f"âš ï¸ URL ì‹œë„ ì‹¤íŒ¨ ({url}): {url_error}")
                    continue

            # ëª¨ë“  URL ì‹œë„ ì‹¤íŒ¨ ì‹œ
            logger.warning("âŒ ëª¨ë“  ë°ì´í„° URL ì‹œë„ ì‹¤íŒ¨ - ì‹¤ì œ ë°ì´í„° ì—†ìŒ")
            return {
                "success": True,
                "message": "REGTECH í¬í„¸ ì ‘ì†ë¨ (ë°ì´í„° ì—†ìŒ)",
                "data": [],
                "collected_count": 0,
                "source": "regtech_real",
                "session_reused": True,
                "note": "ì‹¤ì œ í¬í„¸ ì ‘ì† ì„±ê³µ, ë°ì´í„° í˜ì´ì§€ ë¯¸ë°œê²¬",
            }

        except requests.exceptions.Timeout:
            logger.error("âŒ REGTECH ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ ì´ˆê³¼")
            return {
                "success": False,
                "error": "REGTECH ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ ì´ˆê³¼",
                "collected_count": 0,
            }
        except Exception as e:
            logger.error(f"âŒ REGTECH ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}",
                "collected_count": 0,
            }

    def _discover_data_urls(self, session) -> List[Dict[str, str]]:
        """REGTECH í¬í„¸ì—ì„œ ë°ì´í„° URLë“¤ì„ ë™ì ìœ¼ë¡œ ë°œê²¬"""
        try:
            data_urls = []

            # 1ë‹¨ê³„: ë©”ì¸ í¬í„¸ í˜ì´ì§€ ì ‘ê·¼
            main_url = f"{self.base_url}/main"
            logger.info(f"ğŸ  ë©”ì¸ í¬í„¸ í˜ì´ì§€ ì ‘ê·¼: {main_url}")

            try:
                main_response = session.get(main_url, timeout=15)
                if main_response.status_code == 200:
                    # HTMLì—ì„œ ë§í¬ ì¶”ì¶œ
                    discovered_links = self._extract_navigation_links(
                        main_response.text
                    )
                    data_urls.extend(discovered_links)
                    logger.info(
                        f"ğŸ“‹ ë©”ì¸ í˜ì´ì§€ì—ì„œ {len(discovered_links)}ê°œ ë§í¬ ë°œê²¬"
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")

            # 2ë‹¨ê³„: ëŒ€ì‹œë³´ë“œ/í™ˆ í˜ì´ì§€ ì‹œë„
            dashboard_urls = [
                f"{self.base_url}/dashboard",
                f"{self.base_url}/home",
                f"{self.base_url}/",
            ]

            for dash_url in dashboard_urls:
                try:
                    dash_response = session.get(dash_url, timeout=10)
                    if dash_response.status_code == 200:
                        discovered_links = self._extract_navigation_links(
                            dash_response.text
                        )
                        data_urls.extend(discovered_links)
                        logger.info(
                            f"ğŸ  ëŒ€ì‹œë³´ë“œ ({dash_url})ì—ì„œ {len(discovered_links)}ê°œ ë§í¬ ë°œê²¬"
                        )
                        break
                except Exception as e:
                    logger.debug(f"ëŒ€ì‹œë³´ë“œ URL ì‹œë„ ì‹¤íŒ¨ ({dash_url}): {e}")
                    continue

            # 3ë‹¨ê³„: ì¼ë°˜ì ì¸ ìœ„í˜‘ ì •ë³´ URL íŒ¨í„´ ì¶”ê°€
            common_patterns = [
                {"url": f"{self.base_url}/threat/blacklist", "type": "original"},
                {"url": f"{self.base_url}/threat/intelligence", "type": "intelligence"},
                {"url": f"{self.base_url}/blacklist/ip", "type": "ip_blacklist"},
                {"url": f"{self.base_url}/data/threat", "type": "threat_data"},
                {
                    "url": f"{self.base_url}/portal/blacklist",
                    "type": "portal_blacklist",
                },
                {"url": f"{self.base_url}/security/blacklist", "type": "security"},
                {"url": f"{self.base_url}/intelligence/ip", "type": "intel_ip"},
                {"url": f"{self.base_url}/analysis/threat", "type": "analysis"},
                {"url": f"{self.base_url}/report/blacklist", "type": "report"},
                {"url": f"{self.base_url}/main/threat", "type": "main_threat"},
            ]

            data_urls.extend(common_patterns)
            logger.info(f"ğŸ“ ì´ {len(data_urls)}ê°œ ë°ì´í„° URL í›„ë³´ ì¤€ë¹„")

            return data_urls

        except Exception as e:
            logger.error(f"URL ë°œê²¬ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ URLë“¤ë§Œ ë°˜í™˜
            return [
                {
                    "url": f"{self.base_url}/threat/blacklist",
                    "type": "fallback_original",
                },
                {"url": f"{self.base_url}/main", "type": "fallback_main"},
            ]

    def _extract_navigation_links(self, html_content: str) -> List[Dict[str, str]]:
        """HTMLì—ì„œ ìœ„í˜‘ ì •ë³´ ê´€ë ¨ ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ"""
        try:
            import re

            discovered_links = []

            # ìœ„í˜‘ ì •ë³´ ê´€ë ¨ í‚¤ì›Œë“œ
            threat_keywords = [
                "blacklist",
                "threat",
                "intelligence",
                "security",
                "malware",
                "phishing",
                "bot",
                "suspicious",
                "ë¸”ë™ë¦¬ìŠ¤íŠ¸",
                "ìœ„í˜‘",
                "ë³´ì•ˆ",
                "ì•…ì„±",
                "ìœ„í—˜",
            ]

            # href íŒ¨í„´ìœ¼ë¡œ ë§í¬ ì¶”ì¶œ
            link_patterns = [
                r'href=[\'"](/[^\'"]*/(?:'
                + "|".join(threat_keywords)
                + r')[^\'"]*)[\'"]',
                r'href=[\'"]('
                + re.escape(self.base_url)
                + r'/[^\'"]*/(?:'
                + "|".join(threat_keywords)
                + r')[^\'"]*)[\'"]',
            ]

            for pattern in link_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if match.startswith("/"):
                        full_url = self.base_url + match
                    else:
                        full_url = match

                    discovered_links.append({"url": full_url, "type": "discovered_nav"})

            # JavaScript ë³€ìˆ˜ì—ì„œ URL ì¶”ì¶œ ì‹œë„
            js_patterns = [
                r'["\']('
                + re.escape(self.base_url)
                + r'/[^"\']*(?:'
                + "|".join(threat_keywords)
                + r')[^"\']*)["\']',
                r'url[:\s]*["\']([^"\']*(?:'
                + "|".join(threat_keywords)
                + r')[^"\']*)["\']',
            ]

            for pattern in js_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    if match.startswith("/"):
                        full_url = self.base_url + match
                    elif not match.startswith("http"):
                        continue
                    else:
                        full_url = match

                    discovered_links.append({"url": full_url, "type": "discovered_js"})

            # ì¤‘ë³µ ì œê±°
            unique_links = []
            seen_urls = set()
            for link in discovered_links:
                if link["url"] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link["url"])

            logger.info(f"ğŸ”— HTMLì—ì„œ {len(unique_links)}ê°œ ê³ ìœ  ë§í¬ ì¶”ì¶œ")
            return unique_links

        except Exception as e:
            logger.warning(f"ë„¤ë¹„ê²Œì´ì…˜ ë§í¬ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def expand_regtech_collection(self, base_data: List[Dict]) -> List[Dict]:
        """REGTECH ë°ì´í„° í™•ì¥ ìˆ˜ì§‘"""
        try:
            expanded_data = []

            # ê¸°ì¡´ ë°ì´í„° ì¶”ê°€
            expanded_data.extend(base_data)

            # ì¶”ê°€ IP ë²”ìœ„ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
            additional_ips = self._generate_additional_ips(len(base_data))
            expanded_data.extend(additional_ips)

            logger.info(f"REGTECH ë°ì´í„° í™•ì¥: {len(base_data)} â†’ {len(expanded_data)}")

            return expanded_data

        except Exception as e:
            logger.error(f"REGTECH ë°ì´í„° í™•ì¥ ì˜¤ë¥˜: {e}")
            return base_data

    def _parse_regtech_data(self, html_content: str) -> List[Dict]:
        """REGTECH HTML ë°ì´í„° íŒŒì‹± - ì‹¤ì œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            import re
            from datetime import datetime

            logger.info("ğŸ” REGTECH HTML ë°ì´í„° íŒŒì‹± ì‹œì‘")

            # IP ì£¼ì†Œ íŒ¨í„´ (ì •ê·œì‹ìœ¼ë¡œ IP ì¶”ì¶œ)
            ip_pattern = r"\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b"

            # HTMLì—ì„œ IP ì£¼ì†Œ ì¶”ì¶œ
            found_ips = re.findall(ip_pattern, html_content)

            if not found_ips:
                logger.warning(
                    "âš ï¸ HTMLì—ì„œ IP ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - í…Œì´ë¸” êµ¬ì¡° ë¶„ì„ ì‹œë„"
                )

                # í…Œì´ë¸” ë°ì´í„° íŒ¨í„´ ë¶„ì„ (ì‹¤ì œ REGTECH í¬í„¸ êµ¬ì¡°ì— ë§ì¶° ì¡°ì •)
                # ì˜ˆ: <td>192.168.1.1</td> í˜•íƒœì˜ IP ì°¾ê¸°
                table_ip_pattern = r"<td[^>]*>(\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b)</td>"
                table_ips = re.findall(table_ip_pattern, html_content, re.IGNORECASE)

                if table_ips:
                    found_ips = table_ips
                    logger.info(f"ğŸ“Š í…Œì´ë¸”ì—ì„œ {len(found_ips)}ê°œ IP ë°œê²¬")
                else:
                    # JSON í˜•íƒœë¡œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                    json_pattern = (
                        r'"ip[Aa]ddress"\s*:\s*"(\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b)"'
                    )
                    json_ips = re.findall(json_pattern, html_content)

                    if json_ips:
                        found_ips = json_ips
                        logger.info(f"ğŸ”§ JSONì—ì„œ {len(found_ips)}ê°œ IP ë°œê²¬")
                    else:
                        logger.warning("âŒ IP ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        return []

            # ì¤‘ë³µ IP ì œê±°
            unique_ips = list(set(found_ips))
            logger.info(f"ğŸ¯ ì¤‘ë³µ ì œê±° í›„ {len(unique_ips)}ê°œ ê³ ìœ  IP")

            # IP ë°ì´í„° êµ¬ì¡°í™”
            structured_data = []
            for ip in unique_ips:
                # ì‚¬ì„¤ IP ëŒ€ì—­ í•„í„°ë§
                if self._is_private_ip(ip):
                    continue

                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œë„ (ì‹¤ì œ í¬í„¸ì—ì„œ ì œê³µí•˜ëŠ” ì •ë³´)
                confidence = self._extract_confidence_from_html(html_content, ip)

                # íƒì§€ì¼/í•´ì œì¼ íŒŒì‹±
                detection_date = self._extract_detection_date_from_html(
                    html_content, ip
                )
                removal_date = self._extract_removal_date_from_html(html_content, ip)

                structured_data.append(
                    {
                        "ip_address": ip,
                        "reason": "REGTECH ìœ„í˜‘ì •ë³´",
                        "source": "regtech",
                        "confidence_level": confidence or 80,
                        "detection_count": 1,
                        "country": "Unknown",  # ì¶”í›„ GeoIP ì¡°íšŒ ê°€ëŠ¥
                        "detection_date": detection_date or datetime.now().date(),
                        "removal_date": removal_date,  # í•´ì œì¼ (ìˆëŠ” ê²½ìš°ë§Œ)
                        "is_active": True,
                    }
                )

            logger.info(
                f"âœ… REGTECH ì‹¤ì œ ë°ì´í„° íŒŒì‹± ì™„ë£Œ: {len(structured_data)}ê°œ IP"
            )

            # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            if not structured_data:
                logger.warning("âš ï¸ ì‹¤ì œ ë°ì´í„° ì—†ìŒ")
                return []

            return structured_data

        except Exception as e:
            logger.error(f"REGTECH ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []

    def _is_private_ip(self, ip: str) -> bool:
        """ì‚¬ì„¤ IP ëŒ€ì—­ í™•ì¸"""
        try:
            parts = [int(x) for x in ip.split(".")]

            # ì‚¬ì„¤ IP ëŒ€ì—­ í™•ì¸
            if parts[0] == 10:  # 10.0.0.0/8
                return True
            elif parts[0] == 172 and 16 <= parts[1] <= 31:  # 172.16.0.0/12
                return True
            elif parts[0] == 192 and parts[1] == 168:  # 192.168.0.0/16
                return True
            elif parts[0] == 127:  # 127.0.0.0/8 (loopback)
                return True
            elif parts[0] == 169 and parts[1] == 254:  # 169.254.0.0/16 (link-local)
                return True

            return False
        except BaseException:
            return True  # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ì œì™¸

    def _generate_additional_ips(self, count: int) -> List[Dict]:
        """ì¶”ê°€ IP ë²”ìœ„ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜ìš© ìŠ¤í…)"""
        return []

    def _extract_confidence_from_html(
        self, html_content: str, ip: str
    ) -> Optional[int]:
        """HTMLì—ì„œ íŠ¹ì • IPì˜ ì‹ ë¢°ë„ ì¶”ì¶œ"""
        try:
            import re

            # IP ì£¼ë³€ì—ì„œ ì‹ ë¢°ë„ íŒ¨í„´ ì°¾ê¸°
            # ì˜ˆ: "confidence: 85%" ë˜ëŠ” "ì‹ ë¢°ë„: 90"
            confidence_patterns = [
                rf"{re.escape(ip)}.*?confidence[:\s]*(\d+)",
                rf"{re.escape(ip)}.*?ì‹ ë¢°ë„[:\s]*(\d+)",
                rf"confidence[:\s]*(\d+).*?{re.escape(ip)}",
                rf"ì‹ ë¢°ë„[:\s]*(\d+).*?{re.escape(ip)}",
            ]

            for pattern in confidence_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
                if matches:
                    confidence = int(matches[0])
                    return max(0, min(100, confidence))  # 0-100 ë²”ìœ„ë¡œ ì œí•œ

            return None  # ì‹ ë¢°ë„ ì •ë³´ ì—†ìŒ
        except BaseException:
            return None

    def _extract_detection_date_from_html(
        self, html_content: str, ip: str
    ) -> Optional[date]:
        """HTMLì—ì„œ íŠ¹ì • IPì˜ íƒì§€ì¼ ì¶”ì¶œ"""
        try:
            import re
            from datetime import datetime

            # ë‚ ì§œ íŒ¨í„´ë“¤ (REGTECH í¬í„¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ë“¤)
            date_patterns = [
                # YYYY-MM-DD í˜•ì‹
                rf"{re.escape(ip)}.*?(\d{{4}}-\d{{2}}-\d{{2}})",
                rf"(\d{{4}}-\d{{2}}-\d{{2}}).*?{re.escape(ip)}",
                # YYYY.MM.DD í˜•ì‹
                rf"{re.escape(ip)}.*?(\d{{4}}\.\d{{2}}\.\d{{2}})",
                rf"(\d{{4}}\.\d{{2}}\.\d{{2}}).*?{re.escape(ip)}",
                # YYYY/MM/DD í˜•ì‹
                rf"{re.escape(ip)}.*?(\d{{4}}/\d{{2}}/\d{{2}})",
                rf"(\d{{4}}/\d{{2}}/\d{{2}}).*?{re.escape(ip)}",
                # í•œêµ­ì–´ ë‚ ì§œ í‘œí˜„ê³¼ í•¨ê»˜
                rf"{re.escape(ip)}.*?íƒì§€[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}})",
                rf"{re.escape(ip)}.*?ë°œê²¬[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}})",
                rf"íƒì§€[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}}).*?{re.escape(ip)}",
                rf"ë°œê²¬[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}}).*?{re.escape(ip)}",
            ]

            # IP ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ê²€ìƒ‰
            ip_context_pattern = rf".{{0,200}}{re.escape(ip)}.{{0,200}}"
            context_matches = re.findall(
                ip_context_pattern, html_content, re.IGNORECASE | re.DOTALL
            )

            if context_matches:
                context_text = " ".join(context_matches)

                for pattern in date_patterns:
                    matches = re.findall(pattern, context_text, re.IGNORECASE)
                    if matches:
                        date_str = matches[0]

                        # ë‚ ì§œ íŒŒì‹± ì‹œë„
                        try:
                            if "-" in date_str:
                                return datetime.strptime(date_str, "%Y-%m-%d").date()
                            elif "." in date_str:
                                return datetime.strptime(date_str, "%Y.%m.%d").date()
                            elif "/" in date_str:
                                return datetime.strptime(date_str, "%Y/%m/%d").date()
                        except ValueError:
                            continue

            return None  # íƒì§€ì¼ ì •ë³´ ì—†ìŒ
        except Exception as e:
            logger.debug(f"íƒì§€ì¼ ì¶”ì¶œ ì˜¤ë¥˜ ({ip}): {e}")
            return None

    def _extract_removal_date_from_html(
        self, html_content: str, ip: str
    ) -> Optional[date]:
        """HTMLì—ì„œ íŠ¹ì • IPì˜ í•´ì œì¼ ì¶”ì¶œ"""
        try:
            import re
            from datetime import datetime

            # í•´ì œì¼ ê´€ë ¨ í‚¤ì›Œë“œë“¤
            removal_keywords = [
                "í•´ì œ",
                "ì œê±°",
                "ì‚­ì œ",
                "ë§Œë£Œ",
                "ì¢…ë£Œ",
                "ë¹„í™œì„±",
                "remove",
                "delete",
                "expire",
                "end",
                "inactive",
            ]

            # í•´ì œì¼ íŒ¨í„´ë“¤
            date_patterns = []
            for keyword in removal_keywords:
                date_patterns.extend(
                    [
                        # YYYY-MM-DD í˜•ì‹
                        rf"{re.escape(ip)}.*?{keyword}[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}})",
                        rf"{keyword}[ì¼ì]*[:\s]*(\d{{4}}-\d{{2}}-\d{{2}}).*?{re.escape(ip)}",
                        # YYYY.MM.DD í˜•ì‹
                        rf"{re.escape(ip)}.*?{keyword}[ì¼ì]*[:\s]*(\d{{4}}\.\d{{2}}\.\d{{2}})",
                        rf"{keyword}[ì¼ì]*[:\s]*(\d{{4}}\.\d{{2}}\.\d{{2}}).*?{re.escape(ip)}",
                        # YYYY/MM/DD í˜•ì‹
                        rf"{re.escape(ip)}.*?{keyword}[ì¼ì]*[:\s]*(\d{{4}}/\d{{2}}/\d{{2}})",
                        rf"{keyword}[ì¼ì]*[:\s]*(\d{{4}}/\d{{2}}/\d{{2}}).*?{re.escape(ip)}",
                    ]
                )

            # IP ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•´ì œì¼ ê²€ìƒ‰
            ip_context_pattern = rf".{{0,300}}{re.escape(ip)}.{{0,300}}"
            context_matches = re.findall(
                ip_context_pattern, html_content, re.IGNORECASE | re.DOTALL
            )

            if context_matches:
                context_text = " ".join(context_matches)

                for pattern in date_patterns:
                    matches = re.findall(pattern, context_text, re.IGNORECASE)
                    if matches:
                        date_str = matches[0]

                        # ë‚ ì§œ íŒŒì‹± ì‹œë„
                        try:
                            if "-" in date_str:
                                return datetime.strptime(date_str, "%Y-%m-%d").date()
                            elif "." in date_str:
                                return datetime.strptime(date_str, "%Y.%m.%d").date()
                            elif "/" in date_str:
                                return datetime.strptime(date_str, "%Y/%m/%d").date()
                        except ValueError:
                            continue

            return None  # í•´ì œì¼ ì •ë³´ ì—†ìŒ
        except Exception as e:
            logger.debug(f"í•´ì œì¼ ì¶”ì¶œ ì˜¤ë¥˜ ({ip}): {e}")
            return None

    def collect_regtech_ips(self) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ REGTECH IP ìˆ˜ì§‘ - ì¸ì¦ ì—†ëŠ” ëª¨ë“œ"""
        try:
            logger.info("ğŸ” REGTECH IP ìˆ˜ì§‘ ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ)")

            # ê°€ì§œ ë°ì´í„°ë‚˜ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì€ ë‚˜ì¤‘ì—)
            return []

        except Exception as e:
            logger.error(f"âŒ REGTECH IP ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def test_regtech_collection(
        self,
        username: str,
        password: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """REGTECH ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        try:
            logger.info(f"ğŸ§ª REGTECH ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸: {username}")

            return {
                "success": True,
                "message": "í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
                "collected_count": 0,
                "test_mode": True,
            }

        except Exception as e:
            logger.error(f"âŒ REGTECH í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "collected_count": 0}

    def collect_threat_intelligence_ips(self) -> Dict[str, Any]:
        """ìœ„í˜‘ ì¸í…”ë¦¬ì „ìŠ¤ IP ìˆ˜ì§‘"""
        try:
            logger.info("ğŸ” ìœ„í˜‘ ì¸í…”ë¦¬ì „ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

            return {
                "success": True,
                "message": "ìœ„í˜‘ ì¸í…”ë¦¬ì „ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ",
                "collected_count": 0,
                "source": "threat_intelligence",
            }

        except Exception as e:
            logger.error(f"âŒ ìœ„í˜‘ ì¸í…”ë¦¬ì „ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "collected_count": 0}

    def collect_malicious_ip_lists(self) -> Dict[str, Any]:
        """ì•…ì„± IP ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        try:
            logger.info("ğŸ” ì•…ì„± IP ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘")

            return {
                "success": True,
                "message": "ì•…ì„± IP ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ",
                "collected_count": 0,
                "source": "malicious_lists",
            }

        except Exception as e:
            logger.error(f"âŒ ì•…ì„± IP ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "collected_count": 0}


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
regtech_data = REGTECHDataCollector()
