"""
íƒì§€ì¼ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” API
Detection Date Analytics and Visualization

Updated: 2025-11-21 (Error Handling Standardization - HIGH PRIORITY #4)
Reference: docs/104-ERROR-HANDLING-STANDARDIZATION-PLAN.md
"""

import logging
from datetime import datetime
from flask import Blueprint, jsonify, request, g, render_template, current_app
from ...exceptions import (
    ValidationError,
    DatabaseError,
    InternalServerError,
)

logger = logging.getLogger(__name__)

# Detection Analytics Blueprint
detection_bp = Blueprint("detection_analytics", __name__, url_prefix="/analytics")


@detection_bp.route("/overview", methods=["GET"])
def get_analytics_overview():
    """Analytics dashboard overview with key metrics"""
    try:
        db_service = current_app.extensions["db_service"]
        
        stats = db_service.query("""
            SELECT 
                (SELECT COUNT(*) FROM blacklist_ips) as total_ips,
                (SELECT COUNT(*) FROM blacklist_ips WHERE is_active = true) as active_ips,
                (SELECT COUNT(DISTINCT data_source) FROM blacklist_ips) as source_count,
                (SELECT COUNT(*) FROM blacklist_ips WHERE created_at > NOW() - INTERVAL '24 hours') as new_today,
                (SELECT COUNT(*) FROM blacklist_ips WHERE created_at > NOW() - INTERVAL '7 days') as new_week
        """)
        
        row = stats[0] if stats else {}
        return jsonify({
            "success": True,
            "data": {
                "total_ips": row.get("total_ips", 0),
                "active_ips": row.get("active_ips", 0),
                "source_count": row.get("source_count", 0),
                "new_today": row.get("new_today", 0),
                "new_week": row.get("new_week", 0)
            },
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"Analytics overview failed: {e}")
        raise DatabaseError(
            message="Failed to get analytics overview",
            query="analytics_overview"
        )


@detection_bp.route("/detection-timeline", methods=["GET"])
def get_detection_timeline():
    """
    íƒì§€ì¼ë³„ IP ìˆ˜ì§‘ í˜„í™© ë¶„ì„ (Phase 1.4: Standardized Error Handling)

    GET /analytics/detection-timeline?days=30&format=json

    Query Parameters:
        days (int): Analysis period in days (default: 30)
        format (str): Response format - "json" or "chart" (default: "json")

    Raises:
        ValidationError: Invalid days parameter
        DatabaseError: Database query failed
    """
    # Get and validate query parameters
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
    try:
        days_back = int(request.args.get("days", 365))
    except ValueError as e:
        raise ValidationError(
            message="Days parameter must be a valid integer",
            field="days",
            details={"error": str(e)},
        )
=======
    days_param = request.args.get("days", "30")
    
    # days=0 or days=all means no date filter (all data)
    if days_param.lower() == "all" or days_param == "0":
        days_back = None  # No limit
    else:
        try:
            days_back = int(days_param)
        except ValueError as e:
            raise ValidationError(
                message="Days parameter must be a valid integer or 'all'",
                field="days",
                details={"error": str(e)},
            )
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py

    format_type = request.args.get("format", "json")  # json or chart

    try:
        db_service = current_app.extensions["db_service"]

        conn = db_service.get_connection()
        cursor = conn.cursor()

        # ë‚ ì§œë³„ ìˆ˜ì§‘ í†µê³„ (View ì‚¬ìš© - 3ê°œì›” ìë™ ë¹„í™œì„±í™” ë¡œì§ ì ìš©)
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
        query = """
            SELECT
                COALESCE(detection_date, created_at::date) as detection_day,
                COUNT(*) as ip_count,
                COUNT(DISTINCT source) as source_count,
                STRING_AGG(DISTINCT source, ', ') as sources,
                MIN(created_at) as first_collected,
                MAX(created_at) as last_collected
            FROM blacklist_ips_with_auto_inactive
            WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
            GROUP BY COALESCE(detection_date, created_at::date)
            ORDER BY detection_day DESC
        """

        cursor.execute(query, (days_back,))
=======
        if days_back is None:
            # ì „ì²´ ë°ì´í„° ì¡°íšŒ (ë‚ ì§œ í•„í„° ì—†ìŒ)
            query = """
                SELECT
                    COALESCE(detection_date, created_at::date) as detection_day,
                    COUNT(*) as ip_count,
                    COUNT(DISTINCT source) as source_count,
                    STRING_AGG(DISTINCT source, ', ') as sources,
                    MIN(created_at) as first_collected,
                    MAX(created_at) as last_collected
                FROM blacklist_ips_with_auto_inactive
                GROUP BY COALESCE(detection_date, created_at::date)
                ORDER BY detection_day DESC
            """
            cursor.execute(query)
        else:
            query = """
                SELECT
                    COALESCE(detection_date, created_at::date) as detection_day,
                    COUNT(*) as ip_count,
                    COUNT(DISTINCT source) as source_count,
                    STRING_AGG(DISTINCT source, ', ') as sources,
                    MIN(created_at) as first_collected,
                    MAX(created_at) as last_collected
                FROM blacklist_ips_with_auto_inactive
                WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
                GROUP BY COALESCE(detection_date, created_at::date)
                ORDER BY detection_day DESC
            """
            cursor.execute(query, (days_back,))
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
        results = cursor.fetchall()

        # ì»¬ëŸ¼ëª… ë§¤í•‘
        columns = [
            "detection_day",
            "ip_count",
            "source_count",
            "sources",
            "first_collected",
            "last_collected",
        ]
        timeline_data = []

        for row in results:
            data = dict(zip(columns, row))

            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            if data["detection_day"]:
                data["detection_day"] = str(data["detection_day"])
            if data["first_collected"]:
                data["first_collected"] = data["first_collected"].isoformat()
            if data["last_collected"]:
                data["last_collected"] = data["last_collected"].isoformat()

<<<<<<< Updated upstream:app/core/routes/api/analytics.py
            # ìˆ˜ìƒí•œ íŒ¨í„´ íƒì§€
            suspicious_patterns = []

            # 1. ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ IP (í‰ê· ì˜ 3ë°° ì´ìƒ)
            if len(timeline_data) > 0:
                avg_count = sum([d["ip_count"] for d in timeline_data]) / len(timeline_data)
                if data["ip_count"] > avg_count * 3:
                    suspicious_patterns.append("abnormal_volume")

            # 2. ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì (1000, 5000, 10000 ë“±)
            if data["ip_count"] % 1000 == 0 and data["ip_count"] >= 1000:
                suspicious_patterns.append("round_number")

            # 3. ë™ì¼í•œ ì†ŒìŠ¤ì—ì„œ ëŒ€ëŸ‰ ìˆ˜ì§‘
            if data["source_count"] == 1 and data["ip_count"] > 1000:
                suspicious_patterns.append("single_source_bulk")

            data["suspicious_patterns"] = suspicious_patterns
            data["is_suspicious"] = len(suspicious_patterns) > 0

=======
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
            timeline_data.append(data)

        # í†µê³„ ìš”ì•½
        total_ips = sum([d["ip_count"] for d in timeline_data])
        total_days = len(timeline_data)
        avg_per_day = total_ips / total_days if total_days > 0 else 0

        # ì†ŒìŠ¤ë³„ í†µê³„
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
        cursor.execute(
            """
            SELECT
                source,
                COUNT(*) as total_ips,
                COUNT(DISTINCT COALESCE(detection_date, created_at::date)) as active_days,
                MIN(COALESCE(detection_date, created_at::date)) as first_detection,
                MAX(COALESCE(detection_date, created_at::date)) as last_detection
            FROM blacklist_ips_with_auto_inactive
            WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
            GROUP BY source
            ORDER BY total_ips DESC
        """,
            (days_back,),
        )
=======
        if days_back is None:
            cursor.execute(
                """
                SELECT
                    source,
                    COUNT(*) as total_ips,
                    COUNT(DISTINCT COALESCE(detection_date, created_at::date)) as active_days,
                    MIN(COALESCE(detection_date, created_at::date)) as first_detection,
                    MAX(COALESCE(detection_date, created_at::date)) as last_detection
                FROM blacklist_ips_with_auto_inactive
                GROUP BY source
                ORDER BY total_ips DESC
            """
            )
        else:
            cursor.execute(
                """
                SELECT
                    source,
                    COUNT(*) as total_ips,
                    COUNT(DISTINCT COALESCE(detection_date, created_at::date)) as active_days,
                    MIN(COALESCE(detection_date, created_at::date)) as first_detection,
                    MAX(COALESCE(detection_date, created_at::date)) as last_detection
                FROM blacklist_ips_with_auto_inactive
                WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
                GROUP BY source
                ORDER BY total_ips DESC
            """,
                (days_back,),
            )
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py

        source_results = cursor.fetchall()
        source_stats = []
        for row in source_results:
            source_data = {
                "source": row[0],
                "total_ips": row[1],
                "active_days": row[2],
                "first_detection": str(row[3]) if row[3] else None,
                "last_detection": str(row[4]) if row[4] else None,
                "avg_per_day": round(row[1] / row[2], 1) if row[2] > 0 else 0,
            }
            source_stats.append(source_data)

        cursor.close()
        conn.close()

<<<<<<< Updated upstream:app/core/routes/api/analytics.py
        # ìˆ˜ìƒí•œ íŒ¨í„´ ìš”ì•½
        suspicious_days = [d for d in timeline_data if d["is_suspicious"]]
        pattern_summary = {}
        for day in suspicious_days:
            for pattern in day["suspicious_patterns"]:
                pattern_summary[pattern] = pattern_summary.get(pattern, 0) + 1

        # ë¡œê·¸ ì¶œë ¥ (íƒì§€ì¼ ë°ì´í„° ë¶„ì„)
        logger.info("ğŸ“Š íƒì§€ì¼ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        logger.info(f"   â€¢ ë¶„ì„ ê¸°ê°„: {days_back}ì¼")
        logger.info(f"   â€¢ ì´ IP ìˆ˜: {total_ips:,}ê°œ")
        logger.info(f"   â€¢ í™œì„± ì¼ìˆ˜: {total_days}ì¼")
        logger.info(f"   â€¢ ì¼í‰ê· : {avg_per_day:.1f}ê°œ")
        logger.info(f"   â€¢ ìˆ˜ìƒí•œ íŒ¨í„´: {len(suspicious_days)}ì¼")

        if suspicious_days:
            logger.warning("ğŸš¨ ìˆ˜ìƒí•œ ë°ì´í„° íŒ¨í„´ ë°œê²¬:")
            for day in suspicious_days[:5]:  # ìµœëŒ€ 5ì¼ë§Œ ë¡œê·¸
                logger.warning(
                    f"   â€¢ {day['detection_day']}: {day['ip_count']:,}ê°œ IP ({', '.join(day['suspicious_patterns'])})"
                )
=======
        # ë¡œê·¸ ì¶œë ¥ (íƒì§€ì¼ ë°ì´í„° ë¶„ì„)
        period_str = "ì „ì²´" if days_back is None else f"{days_back}ì¼"
        logger.info("ğŸ“Š íƒì§€ì¼ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        logger.info(f"   â€¢ ë¶„ì„ ê¸°ê°„: {period_str}")
        logger.info(f"   â€¢ ì´ IP ìˆ˜: {total_ips:,}ê°œ")
        logger.info(f"   â€¢ í™œì„± ì¼ìˆ˜: {total_days}ì¼")
        logger.info(f"   â€¢ ì¼í‰ê· : {avg_per_day:.1f}ê°œ")
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py

        response_data = {
            "success": True,
            "metadata": {
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
                "analysis_period_days": days_back,
=======
                "analysis_period_days": days_back,  # None means all
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
                "total_ips": total_ips,
                "total_days": total_days,
                "avg_per_day": round(avg_per_day, 1),
                "generated_at": datetime.now().isoformat(),
            },
            "timeline": timeline_data,
            "source_statistics": source_stats,
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
            "suspicious_analysis": {
                "suspicious_days_count": len(suspicious_days),
                "pattern_summary": pattern_summary,
                "suspicious_days": suspicious_days[:10],  # ìµœëŒ€ 10ì¼ë§Œ ë°˜í™˜
            },
=======
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
        }

        if format_type == "chart":
            # ì°¨íŠ¸ìš© ê°„ë‹¨í•œ ë°ì´í„° í˜•ì‹
            chart_data = {
                "labels": [d["detection_day"] for d in timeline_data],
                "datasets": [
                    {
                        "label": "IP ìˆ˜ì§‘ëŸ‰",
                        "data": [d["ip_count"] for d in timeline_data],
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
                        "backgroundColor": [
                            ("rgba(255, 99, 132, 0.8)" if d["is_suspicious"] else "rgba(54, 162, 235, 0.8)")
                            for d in timeline_data
                        ],
                        "borderColor": [
                            ("rgba(255, 99, 132, 1)" if d["is_suspicious"] else "rgba(54, 162, 235, 1)")
                            for d in timeline_data
                        ],
=======
                        "backgroundColor": "rgba(54, 162, 235, 0.8)",
                        "borderColor": "rgba(54, 162, 235, 1)",
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
                        "borderWidth": 1,
                    }
                ],
            }
            return jsonify(
                {
                    "success": True,
                    "data": {
                        "chart_data": chart_data,
                        "summary": response_data["metadata"],
<<<<<<< Updated upstream:app/core/routes/api/analytics.py
                        "suspicious_count": len(suspicious_days),
=======
>>>>>>> Stashed changes:app-source/core/routes/api/analytics.py
                    },
                    "timestamp": datetime.now().isoformat(),
                    "request_id": g.request_id,
                }
            ), 200

        return jsonify(
            {
                "success": True,
                "data": response_data,
                "timestamp": datetime.now().isoformat(),
                "request_id": g.request_id,
            }
        ), 200

    except Exception as e:
        logger.error(f"Detection timeline analysis error: {e}", exc_info=True)
        raise DatabaseError(
            message="Failed to analyze detection timeline",
            details={
                "days": days_back,
                "format": format_type,
                "error_type": type(e).__name__,
            },
        )


@detection_bp.route("/suspicious-patterns", methods=["GET"])
def get_suspicious_patterns():
    """
    ìˆ˜ìƒí•œ ë°ì´í„° íŒ¨í„´ ìƒì„¸ ë¶„ì„ (Phase 1.4: Standardized Error Handling)

    GET /analytics/suspicious-patterns

    Raises:
        DatabaseError: Database query failed
    """
    db_service = current_app.extensions["db_service"]
    conn = None

    try:
        conn = db_service.get_connection()
        cursor = conn.cursor()

        try:
            patterns = []

            # 1. ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì íŒ¨í„´
            cursor.execute(
                """
                SELECT
                    COALESCE(detection_date, created_at::date) as day,
                    COUNT(*) as count,
                    source
                FROM blacklist_ips_with_auto_inactive
                GROUP BY COALESCE(detection_date, created_at::date), source
                HAVING COUNT(*) % 1000 = 0 AND COUNT(*) >= 1000
                ORDER BY count DESC
            """
            )

            round_numbers = cursor.fetchall()
            if round_numbers:
                patterns.append(
                    {
                        "type": "round_numbers",
                        "description": "ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì (1000, 5000, 10000 ë“±)",
                        "severity": "high",
                        "count": len(round_numbers),
                        "examples": [
                            {"date": str(row[0]), "ip_count": row[1], "source": row[2]} for row in round_numbers[:5]
                        ],
                    }
                )

                # ë¡œê·¸ ì¶œë ¥
                logger.warning("ğŸš¨ ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì íŒ¨í„´ ë°œê²¬:")
                for row in round_numbers[:3]:
                    logger.warning(f"   â€¢ {row[0]}: {row[1]:,}ê°œ ({row[2]})")

            # 2. ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘
            cursor.execute(
                """
                WITH daily_stats AS (
                    SELECT
                        COALESCE(detection_date, created_at::date) as day,
                        COUNT(*) as count
                    FROM blacklist_ips_with_auto_inactive
                    GROUP BY COALESCE(detection_date, created_at::date)
                ),
                avg_stats AS (
                    SELECT AVG(count) as avg_count, STDDEV(count) as stddev_count
                    FROM daily_stats
                )
                SELECT d.day, d.count, a.avg_count
                FROM daily_stats d, avg_stats a
                WHERE d.count > (a.avg_count + 2 * COALESCE(a.stddev_count, a.avg_count))
                ORDER BY d.count DESC
            """
            )

            bulk_collections = cursor.fetchall()
            if bulk_collections:
                patterns.append(
                    {
                        "type": "bulk_collection",
                        "description": "ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘ (í‰ê·  + 2Ïƒ ì´ìƒ)",
                        "severity": "medium",
                        "count": len(bulk_collections),
                        "examples": [
                            {
                                "date": str(row[0]),
                                "ip_count": row[1],
                                "avg_baseline": round(float(row[2]), 1),
                            }
                            for row in bulk_collections[:5]
                        ],
                    }
                )

                # ë¡œê·¸ ì¶œë ¥
                logger.warning("ğŸš¨ ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒ¨í„´ ë°œê²¬:")
                for row in bulk_collections[:3]:
                    logger.warning(f"   â€¢ {row[0]}: {row[1]:,}ê°œ (í‰ê· : {row[2]:.1f}ê°œ)")

            # 3. ë‹¨ì¼ ì†ŒìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘
            cursor.execute(
                """
                SELECT
                    data_source,
                    COALESCE(detection_date, created_at::date) as day,
                    COUNT(*) as count
                FROM blacklist_ips_with_auto_inactive
                GROUP BY data_source, COALESCE(detection_date, created_at::date)
                HAVING COUNT(*) > 10000
                ORDER BY count DESC
            """
            )

            single_source_bulk = cursor.fetchall()
            if single_source_bulk:
                patterns.append(
                    {
                        "type": "single_source_bulk",
                        "description": "ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ 1ë§Œê°œ ì´ìƒ ìˆ˜ì§‘",
                        "severity": "high",
                        "count": len(single_source_bulk),
                        "examples": [
                            {"source": row[0], "date": str(row[1]), "ip_count": row[2]}
                            for row in single_source_bulk[:5]
                        ],
                    }
                )

                # ë¡œê·¸ ì¶œë ¥
                logger.warning("ğŸš¨ ë‹¨ì¼ ì†ŒìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒ¨í„´ ë°œê²¬:")
                for row in single_source_bulk[:3]:
                    logger.warning(f"   â€¢ {row[1]} {row[0]}: {row[2]:,}ê°œ")

            # ì¢…í•© ìœ„í—˜ë„ í‰ê°€
            total_issues = sum([p["count"] for p in patterns])
            risk_level = "low"
            if total_issues > 10:
                risk_level = "high"
            elif total_issues > 5:
                risk_level = "medium"

            logger.info(
                f"ğŸ“Š ìˆ˜ìƒí•œ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {len(patterns)}ê°€ì§€ íŒ¨í„´, ì´ {total_issues}ê±´, ìœ„í—˜ë„: {risk_level}"
            )

            return jsonify(
                {
                    "success": True,
                    "data": {
                        "analysis": {
                            "total_pattern_types": len(patterns),
                            "total_issues": total_issues,
                            "risk_level": risk_level,
                            "generated_at": datetime.now().isoformat(),
                        },
                        "patterns": patterns,
                    },
                    "timestamp": datetime.now().isoformat(),
                    "request_id": g.request_id,
                }
            ), 200

        finally:
            cursor.close()

    except Exception as e:
        logger.error(f"Suspicious patterns analysis error: {e}", exc_info=True)
        raise DatabaseError(
            message="Failed to analyze suspicious patterns",
            details={"error_type": type(e).__name__},
        )
    finally:
        if conn:
            db_service.return_connection(conn)


@detection_bp.route("/detection-chart")
def detection_chart_page():
    """
    íƒì§€ì¼ ë°ì´í„° ì‹œê°í™” í˜ì´ì§€ (Phase 1.4: Standardized Error Handling)

    GET /analytics/detection-chart

    Raises:
        InternalServerError: Template rendering failed
    """
    try:
        return render_template("detection_chart.html")
    except Exception as e:
        logger.error(f"Detection chart page error: {e}", exc_info=True)
        raise InternalServerError(
            message="Failed to load detection chart page",
            details={"error_type": type(e).__name__},
        )


@detection_bp.route("/real-time-log", methods=["GET"])
def get_real_time_detection_log():
    """
    ì‹¤ì‹œê°„ íƒì§€ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ (Phase 1.4: Standardized Error Handling)

    GET /analytics/real-time-log

    Raises:
        DatabaseError: Database query failed
    """
    try:
        db_service = current_app.extensions["db_service"]

        # ìµœê·¼ 24ì‹œê°„ ë‚´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œê·¸
        conn = db_service.get_connection()
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT
                created_at,
                ip_address,
                source,
                COALESCE(detection_date, created_at::date) as detection_day,
                confidence_level
            FROM blacklist_ips_with_auto_inactive
            WHERE created_at >= NOW() - INTERVAL '24 hours'
            ORDER BY created_at DESC
            LIMIT 10000
        """
        )

        results = cursor.fetchall()

        log_entries = []
        for row in results:
            entry = {
                "timestamp": row[0].isoformat() if row[0] else None,
                "ip_address": row[1],
                "source": row[2],
                "detection_day": str(row[3]) if row[3] else None,
                "confidence_level": row[4],
                "log_type": "detection",
            }
            log_entries.append(entry)

        cursor.close()
        conn.close()

        # ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“¡ ì‹¤ì‹œê°„ íƒì§€ ë¡œê·¸: ìµœê·¼ 24ì‹œê°„ {len(log_entries)}ê±´")
        for entry in log_entries[:5]:  # ìµœê·¼ 5ê±´ë§Œ ë¡œê·¸
            logger.info(f"   â€¢ {entry['timestamp']}: {entry['ip_address']} ({entry['source']})")

        return jsonify(
            {
                "success": True,
                "data": {
                    "log_entries": log_entries,
                    "metadata": {
                        "total_entries": len(log_entries),
                        "time_range": "24h",
                        "generated_at": datetime.now().isoformat(),
                    },
                },
                "timestamp": datetime.now().isoformat(),
                "request_id": g.request_id,
            }
        ), 200

    except Exception as e:
        logger.error(f"Real-time detection log error: {e}", exc_info=True)
        raise DatabaseError(
            message="Failed to retrieve real-time detection log",
            details={"error_type": type(e).__name__},
        )
