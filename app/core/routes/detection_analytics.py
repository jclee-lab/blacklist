"""
íƒì§€ì¼ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” API
Detection Date Analytics and Visualization
"""

import logging
from datetime import datetime
from flask import Blueprint, jsonify, request, render_template

logger = logging.getLogger(__name__)

# Detection Analytics Blueprint
detection_bp = Blueprint("detection_analytics", __name__, url_prefix="/analytics")


@detection_bp.route("/detection-timeline", methods=["GET"])
def get_detection_timeline():
    """íƒì§€ì¼ë³„ IP ìˆ˜ì§‘ í˜„í™© ë¶„ì„"""
    try:
        from ..services.database_service import db_service

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
        days_back = int(request.args.get("days", 30))  # ê¸°ë³¸ 30ì¼
        format_type = request.args.get("format", "json")  # json ë˜ëŠ” chart

        conn = db_service.get_connection()
        cursor = conn.cursor()

        # ë‚ ì§œë³„ ìˆ˜ì§‘ í†µê³„ (detection_dateê°€ ì—†ìœ¼ë©´ created_at ì‚¬ìš©)
        query = """
            SELECT
                COALESCE(detection_date, created_at::date) as detection_day,
                COUNT(*) as ip_count,
                COUNT(DISTINCT source) as source_count,
                STRING_AGG(DISTINCT source, ', ') as sources,
                MIN(created_at) as first_collected,
                MAX(created_at) as last_collected
            FROM blacklist_ips
            WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
            GROUP BY COALESCE(detection_date, created_at::date)
            ORDER BY detection_day DESC
        """

        cursor.execute(query, (days_back,))
        results = cursor.fetchall()

        # ì»¬ëŸ¼ëª… ë§¤í•‘
        columns = [
            "detection_day",
            "ip_count",
            "source_count",
            "sources",
            "first_collected",
            "last_collected",
        ]
        timeline_data = []

        for row in results:
            data = dict(zip(columns, row))

            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            if data["detection_day"]:
                data["detection_day"] = str(data["detection_day"])
            if data["first_collected"]:
                data["first_collected"] = data["first_collected"].isoformat()
            if data["last_collected"]:
                data["last_collected"] = data["last_collected"].isoformat()

            # ìˆ˜ìƒí•œ íŒ¨í„´ íƒì§€
            suspicious_patterns = []

            # 1. ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ IP (í‰ê· ì˜ 3ë°° ì´ìƒ)
            if len(timeline_data) > 0:
                avg_count = sum([d["ip_count"] for d in timeline_data]) / len(
                    timeline_data
                )
                if data["ip_count"] > avg_count * 3:
                    suspicious_patterns.append("abnormal_volume")

            # 2. ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì (1000, 5000, 10000 ë“±)
            if data["ip_count"] % 1000 == 0 and data["ip_count"] >= 1000:
                suspicious_patterns.append("round_number")

            # 3. ë™ì¼í•œ ì†ŒìŠ¤ì—ì„œ ëŒ€ëŸ‰ ìˆ˜ì§‘
            if data["source_count"] == 1 and data["ip_count"] > 1000:
                suspicious_patterns.append("single_source_bulk")

            data["suspicious_patterns"] = suspicious_patterns
            data["is_suspicious"] = len(suspicious_patterns) > 0

            timeline_data.append(data)

        # í†µê³„ ìš”ì•½
        total_ips = sum([d["ip_count"] for d in timeline_data])
        total_days = len(timeline_data)
        avg_per_day = total_ips / total_days if total_days > 0 else 0

        # ì†ŒìŠ¤ë³„ í†µê³„
        cursor.execute(
            """
            SELECT
                source,
                COUNT(*) as total_ips,
                COUNT(DISTINCT COALESCE(detection_date, created_at::date)) as active_days,
                MIN(COALESCE(detection_date, created_at::date)) as first_detection,
                MAX(COALESCE(detection_date, created_at::date)) as last_detection
            FROM blacklist_ips
            WHERE COALESCE(detection_date, created_at::date) >= CURRENT_DATE - INTERVAL '%s days'
            GROUP BY source
            ORDER BY total_ips DESC
        """,
            (days_back,),
        )

        source_results = cursor.fetchall()
        source_stats = []
        for row in source_results:
            source_data = {
                "source": row[0],
                "total_ips": row[1],
                "active_days": row[2],
                "first_detection": str(row[3]) if row[3] else None,
                "last_detection": str(row[4]) if row[4] else None,
                "avg_per_day": round(row[1] / row[2], 1) if row[2] > 0 else 0,
            }
            source_stats.append(source_data)

        cursor.close()
        conn.close()

        # ìˆ˜ìƒí•œ íŒ¨í„´ ìš”ì•½
        suspicious_days = [d for d in timeline_data if d["is_suspicious"]]
        pattern_summary = {}
        for day in suspicious_days:
            for pattern in day["suspicious_patterns"]:
                pattern_summary[pattern] = pattern_summary.get(pattern, 0) + 1

        # ë¡œê·¸ ì¶œë ¥ (íƒì§€ì¼ ë°ì´í„° ë¶„ì„)
        logger.info("ğŸ“Š íƒì§€ì¼ ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        logger.info(f"   â€¢ ë¶„ì„ ê¸°ê°„: {days_back}ì¼")
        logger.info(f"   â€¢ ì´ IP ìˆ˜: {total_ips:,}ê°œ")
        logger.info(f"   â€¢ í™œì„± ì¼ìˆ˜: {total_days}ì¼")
        logger.info(f"   â€¢ ì¼í‰ê· : {avg_per_day:.1f}ê°œ")
        logger.info(f"   â€¢ ìˆ˜ìƒí•œ íŒ¨í„´: {len(suspicious_days)}ì¼")

        if suspicious_days:
            logger.warning("ğŸš¨ ìˆ˜ìƒí•œ ë°ì´í„° íŒ¨í„´ ë°œê²¬:")
            for day in suspicious_days[:5]:  # ìµœëŒ€ 5ì¼ë§Œ ë¡œê·¸
                logger.warning(
                    f"   â€¢ {day['detection_day']}: {day['ip_count']:,}ê°œ IP ({', '.join(day['suspicious_patterns'])})"
                )

        response_data = {
            "success": True,
            "metadata": {
                "analysis_period_days": days_back,
                "total_ips": total_ips,
                "total_days": total_days,
                "avg_per_day": round(avg_per_day, 1),
                "generated_at": datetime.now().isoformat(),
            },
            "timeline": timeline_data,
            "source_statistics": source_stats,
            "suspicious_analysis": {
                "suspicious_days_count": len(suspicious_days),
                "pattern_summary": pattern_summary,
                "suspicious_days": suspicious_days[:10],  # ìµœëŒ€ 10ì¼ë§Œ ë°˜í™˜
            },
        }

        if format_type == "chart":
            # ì°¨íŠ¸ìš© ê°„ë‹¨í•œ ë°ì´í„° í˜•ì‹
            chart_data = {
                "labels": [d["detection_day"] for d in timeline_data],
                "datasets": [
                    {
                        "label": "IP ìˆ˜ì§‘ëŸ‰",
                        "data": [d["ip_count"] for d in timeline_data],
                        "backgroundColor": [
                            (
                                "rgba(255, 99, 132, 0.8)"
                                if d["is_suspicious"]
                                else "rgba(54, 162, 235, 0.8)"
                            )
                            for d in timeline_data
                        ],
                        "borderColor": [
                            (
                                "rgba(255, 99, 132, 1)"
                                if d["is_suspicious"]
                                else "rgba(54, 162, 235, 1)"
                            )
                            for d in timeline_data
                        ],
                        "borderWidth": 1,
                    }
                ],
            }
            return jsonify(
                {
                    "success": True,
                    "chart_data": chart_data,
                    "summary": response_data["metadata"],
                    "suspicious_count": len(suspicious_days),
                }
            )

        return jsonify(response_data)

    except Exception as e:
        logger.error(f"Detection timeline analysis error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@detection_bp.route("/suspicious-patterns", methods=["GET"])
def get_suspicious_patterns():
    """ìˆ˜ìƒí•œ ë°ì´í„° íŒ¨í„´ ìƒì„¸ ë¶„ì„"""
    try:
        import os
        import psycopg2
        from psycopg2.extras import RealDictCursor

        # ì§ì ‘ DB ì—°ê²°ë¡œ ì¸ì¦ ë¬¸ì œ í•´ê²°
        conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "blacklist-postgres"),
            port=os.getenv("POSTGRES_PORT", "5432"),
            database=os.getenv("POSTGRES_DB", "blacklist"),
            user=os.getenv("POSTGRES_USER", "postgres"),
            password=os.getenv("POSTGRES_PASSWORD", "postgres"),
            cursor_factory=RealDictCursor,
        )
        cursor = conn.cursor()

        patterns = []

        # 1. ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì íŒ¨í„´
        cursor.execute(
            """
            SELECT
                COALESCE(detection_date, created_at::date) as day,
                COUNT(*) as count,
                source
            FROM blacklist_ips
            GROUP BY COALESCE(detection_date, created_at::date), source
            HAVING COUNT(*) % 1000 = 0 AND COUNT(*) >= 1000
            ORDER BY count DESC
        """
        )

        round_numbers = cursor.fetchall()
        if round_numbers:
            patterns.append(
                {
                    "type": "round_numbers",
                    "description": "ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì (1000, 5000, 10000 ë“±)",
                    "severity": "high",
                    "count": len(round_numbers),
                    "examples": [
                        {"date": str(row[0]), "ip_count": row[1], "source": row[2]}
                        for row in round_numbers[:5]
                    ],
                }
            )

            # ë¡œê·¸ ì¶œë ¥
            logger.warning("ğŸš¨ ì •í™•íˆ ë–¨ì–´ì§€ëŠ” ìˆ«ì íŒ¨í„´ ë°œê²¬:")
            for row in round_numbers[:3]:
                logger.warning(f"   â€¢ {row[0]}: {row[1]:,}ê°œ ({row[2]})")

        # 2. ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘
        cursor.execute(
            """
            WITH daily_stats AS (
                SELECT
                    COALESCE(detection_date, created_at::date) as day,
                    COUNT(*) as count
                FROM blacklist_ips
                GROUP BY COALESCE(detection_date, created_at::date)
            ),
            avg_stats AS (
                SELECT AVG(count) as avg_count, STDDEV(count) as stddev_count
                FROM daily_stats
            )
            SELECT d.day, d.count, a.avg_count
            FROM daily_stats d, avg_stats a
            WHERE d.count > (a.avg_count + 2 * COALESCE(a.stddev_count, a.avg_count))
            ORDER BY d.count DESC
        """
        )

        bulk_collections = cursor.fetchall()
        if bulk_collections:
            patterns.append(
                {
                    "type": "bulk_collection",
                    "description": "ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘ (í‰ê·  + 2Ïƒ ì´ìƒ)",
                    "severity": "medium",
                    "count": len(bulk_collections),
                    "examples": [
                        {
                            "date": str(row[0]),
                            "ip_count": row[1],
                            "avg_baseline": round(float(row[2]), 1),
                        }
                        for row in bulk_collections[:5]
                    ],
                }
            )

            # ë¡œê·¸ ì¶œë ¥
            logger.warning("ğŸš¨ ë¹„ì •ìƒì  ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒ¨í„´ ë°œê²¬:")
            for row in bulk_collections[:3]:
                logger.warning(f"   â€¢ {row[0]}: {row[1]:,}ê°œ (í‰ê· : {row[2]:.1f}ê°œ)")

        # 3. ë‹¨ì¼ ì†ŒìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘
        cursor.execute(
            """
            SELECT
                source,
                COALESCE(detection_date, created_at::date) as day,
                COUNT(*) as count
            FROM blacklist_ips
            GROUP BY source, COALESCE(detection_date, created_at::date)
            HAVING COUNT(*) > 10000
            ORDER BY count DESC
        """
        )

        single_source_bulk = cursor.fetchall()
        if single_source_bulk:
            patterns.append(
                {
                    "type": "single_source_bulk",
                    "description": "ë‹¨ì¼ ì†ŒìŠ¤ì—ì„œ 1ë§Œê°œ ì´ìƒ ìˆ˜ì§‘",
                    "severity": "high",
                    "count": len(single_source_bulk),
                    "examples": [
                        {"source": row[0], "date": str(row[1]), "ip_count": row[2]}
                        for row in single_source_bulk[:5]
                    ],
                }
            )

            # ë¡œê·¸ ì¶œë ¥
            logger.warning("ğŸš¨ ë‹¨ì¼ ì†ŒìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒ¨í„´ ë°œê²¬:")
            for row in single_source_bulk[:3]:
                logger.warning(f"   â€¢ {row[1]} {row[0]}: {row[2]:,}ê°œ")

        cursor.close()
        conn.close()

        # ì¢…í•© ìœ„í—˜ë„ í‰ê°€
        total_issues = sum([p["count"] for p in patterns])
        risk_level = "low"
        if total_issues > 10:
            risk_level = "high"
        elif total_issues > 5:
            risk_level = "medium"

        logger.info(
            f"ğŸ“Š ìˆ˜ìƒí•œ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {len(patterns)}ê°€ì§€ íŒ¨í„´, ì´ {total_issues}ê±´, ìœ„í—˜ë„: {risk_level}"
        )

        return jsonify(
            {
                "success": True,
                "analysis": {
                    "total_pattern_types": len(patterns),
                    "total_issues": total_issues,
                    "risk_level": risk_level,
                    "generated_at": datetime.now().isoformat(),
                },
                "patterns": patterns,
            }
        )

    except Exception as e:
        logger.error(f"Suspicious patterns analysis error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@detection_bp.route("/detection-chart")
def detection_chart_page():
    """íƒì§€ì¼ ë°ì´í„° ì‹œê°í™” í˜ì´ì§€"""
    try:
        return render_template("detection_chart.html")
    except Exception as e:
        logger.error(f"Detection chart page error: {e}")
        return f"Error loading detection chart: {e}", 500


@detection_bp.route("/real-time-log", methods=["GET"])
def get_real_time_detection_log():
    """ì‹¤ì‹œê°„ íƒì§€ ë¡œê·¸ ìŠ¤íŠ¸ë¦¼"""
    try:
        from ..services.database_service import db_service

        # ìµœê·¼ 24ì‹œê°„ ë‚´ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œê·¸
        conn = db_service.get_connection()
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT
                created_at,
                ip_address,
                source,
                COALESCE(detection_date, created_at::date) as detection_day,
                confidence_level
            FROM blacklist_ips
            WHERE created_at >= NOW() - INTERVAL '24 hours'
            ORDER BY created_at DESC
            LIMIT 10000
        """
        )

        results = cursor.fetchall()

        log_entries = []
        for row in results:
            entry = {
                "timestamp": row[0].isoformat() if row[0] else None,
                "ip_address": row[1],
                "source": row[2],
                "detection_day": str(row[3]) if row[3] else None,
                "confidence_level": row[4],
                "log_type": "detection",
            }
            log_entries.append(entry)

        cursor.close()
        conn.close()

        # ì‹¤ì‹œê°„ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ğŸ“¡ ì‹¤ì‹œê°„ íƒì§€ ë¡œê·¸: ìµœê·¼ 24ì‹œê°„ {len(log_entries)}ê±´")
        for entry in log_entries[:5]:  # ìµœê·¼ 5ê±´ë§Œ ë¡œê·¸
            logger.info(
                f"   â€¢ {entry['timestamp']}: {entry['ip_address']} ({entry['source']})"
            )

        return jsonify(
            {
                "success": True,
                "log_entries": log_entries,
                "metadata": {
                    "total_entries": len(log_entries),
                    "time_range": "24h",
                    "generated_at": datetime.now().isoformat(),
                },
            }
        )

    except Exception as e:
        logger.error(f"Real-time detection log error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
