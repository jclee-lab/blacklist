"""
REGTECH Collector Service
REGTECH í¬í„¸ì—ì„œ ë¸”ë™ë¦¬ìŠ¤íŠ¸ IP ìˆ˜ì§‘

Refactored: 2026-01-05 (Technical Debt Resolution)
- Extracted: regtech_parsers.py (íŒŒì‹± ìœ í‹¸ë¦¬í‹°)
- Extracted: regtech_excel.py (Excel ë‹¤ìš´ë¡œë“œ/íŒŒì‹±)
"""

import logging
import requests
from requests.adapters import HTTPAdapter
import json
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from collector.config import CollectorConfig
from core.rate_limiter import regtech_rate_limiter, auth_rate_limiter
from core.regtech_parsers import (
    parse_html_response,
)
from core.regtech_excel import download_excel_data

logger = logging.getLogger(__name__)


class RegtechCollector:
    """ê³ ì„±ëŠ¥ REGTECH ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤ - ìµœì í™”ëœ ìˆ˜ì§‘ ë° ì²˜ë¦¬"""

    def __init__(self):
        self.base_url = CollectorConfig.REGTECH_BASE_URL
        self.session = requests.Session()

        self.proxy_url = os.getenv("WARP_PROXY_URL")
        if self.proxy_url:
            self.session.proxies = {"http": self.proxy_url, "https": self.proxy_url}
            logger.info(f"ğŸŒ WARP í”„ë¡ì‹œ í™œì„±í™”: {self.proxy_url}")

        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "X-Requested-With": "XMLHttpRequest",
                "Origin": self.base_url,
                "Referer": f"{self.base_url}/login/loginForm",
            }
        )

        # ì„¸ì…˜ ìµœì í™”
        adapter = HTTPAdapter(pool_connections=10, pool_maxsize=20, max_retries=3, pool_block=False)
        self.session.mount("https://", adapter)
        self.session.mount("http://", adapter)

        self.authenticated = False
        self._data_cache = {}
        self._auth_cache = {}
        self._cache_ttl = 3600
        self._jwt_expiry: Optional[float] = None
        self._last_credentials: Optional[tuple] = None

        self.rate_limiter = regtech_rate_limiter
        self.auth_rate_limiter = auth_rate_limiter
        logger.info("ğŸš¦ Rate Limiter í†µí•©: API ì°¨ë‹¨ ë°©ì§€ í™œì„±í™”")

    def _find_member(self, username: str) -> Optional[str]:
        """ì‚¬ìš©ì IDë¡œ ë‚´ë¶€ member_id ì¡°íšŒ (1ë‹¨ê³„ ì¸ì¦)"""
        try:
            url = f"{self.base_url}/login/findOneMember"
            data = {"username": username}

            logger.info(f"ğŸ” ì‚¬ìš©ì ì¡°íšŒ ì‹œë„: {username}")
            response = self.session.post(url, json=data, timeout=10)

            if response.status_code == 200:
                try:
                    result = response.json()
                    if isinstance(result, dict) and "id" in result:
                        member_id = str(result["id"])
                        logger.info(f"âœ… ì‚¬ìš©ì ì¡°íšŒ ì„±ê³µ: {member_id}")
                        return member_id
                    else:
                        logger.warning(f"âš ï¸ ì‚¬ìš©ì ì¡°íšŒ ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜: {result}")
                except Exception as json_err:
                    logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {json_err}")
                    logger.error(f"ğŸ“„ ì‘ë‹µ ë‚´ìš©(ì•ë¶€ë¶„): {response.text[:500]}")
            else:
                logger.warning(f"âš ï¸ ì‚¬ìš©ì ì¡°íšŒ ì‹¤íŒ¨: {response.status_code}")
                logger.warning(f"ğŸ“„ ì‘ë‹µ ë‚´ìš©: {response.text[:200]}")

            return None
        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def authenticate(self, username: str, password: str) -> bool:
        """REGTECH 2ë‹¨ê³„ ì¸ì¦: findOneMember â†’ addLogin"""
        auth_key = f"{username}:{hash(password)}"
        self._last_credentials = (username, password)

        if auth_key in self._auth_cache:
            cache_time, is_valid = self._auth_cache[auth_key]
            if time.time() - cache_time < self._cache_ttl and is_valid:
                if self._is_jwt_valid():
                    self.authenticated = True
                    logger.info("âœ… ìºì‹œëœ REGTECH ì¸ì¦ ì‚¬ìš©")
                    return True
                else:
                    logger.info("ğŸ”„ JWT ë§Œë£Œ - ì¬ì¸ì¦ í•„ìš”")

        if not self.auth_rate_limiter.wait_if_needed():
            logger.warning("ğŸ”’ ì¸ì¦ Rate Limiter ì°¨ë‹¨ (ì ê¸ˆ ìƒíƒœ)")
            return False

        self.session.cookies.clear()

        if self.proxy_url:
            self.session.proxies = {
                "http": self.proxy_url,
                "https": self.proxy_url,
            }

        import urllib.parse

        logger.info(f"ğŸ” REGTECH ë¡œê·¸ì¸ ì‹œë„: {username}")

        try:
            # Step 0: loginForm GET (ì„¸ì…˜ ì¿ í‚¤ íšë“)
            self.session.get(f"{self.base_url}/login/loginForm", timeout=30)

            # Step 1: findOneMember (íšŒì› ê²€ì¦ AJAX - ì‹¤ì œ ë¸Œë¼ìš°ì € íë¦„)
            verify_payload = urllib.parse.urlencode({"memberId": username, "memberPw": password})
            logger.info("ğŸ“¤ Step 1: /member/findOneMember íšŒì› ê²€ì¦")

            verify_response = self.session.post(
                f"{self.base_url}/member/findOneMember",
                data=verify_payload,
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                timeout=30,
            )

            logger.info(f"ğŸ“Š Step 1 ì‘ë‹µ: Status={verify_response.status_code}")

            if verify_response.status_code != 200:
                logger.warning(f"âš ï¸ íšŒì› ê²€ì¦ ì‹¤íŒ¨: HTTP {verify_response.status_code}")
                self.auth_rate_limiter.on_failure(error_code=verify_response.status_code)
                self._auth_cache[auth_key] = (time.time(), False)
                logger.error("âŒ REGTECH ì¸ì¦ ì‹¤íŒ¨")
                return False

            # Step 2: addLogin (ì‹¤ì œ ë¡œê·¸ì¸ form submit)
            login_payload = {
                "username": username,
                "password": password,
                "login_error": "",
                "smsTimeExcess": "N",
                "txId": "",
                "token": "",
                "memberId": "",
            }
            encoded_data = urllib.parse.urlencode(login_payload)
            logger.info("ğŸ“¤ Step 2: /login/addLogin ë¡œê·¸ì¸ ìš”ì²­")

            response = self.session.post(
                f"{self.base_url}/login/addLogin",
                data=encoded_data,
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                timeout=30,
                allow_redirects=False,
            )

            status_code = response.status_code
            location = response.headers.get("Location", "")
            jwt_cookie = None

            for cookie in response.cookies:
                if cookie.name == "regtech-va" and cookie.value:
                    jwt_cookie = cookie.value
                    self.session.cookies.set("regtech-va", jwt_cookie, domain="regtech.fsec.or.kr")
                elif cookie.name == "regtech-front" and cookie.value:
                    self.session.cookies.set("regtech-front", cookie.value, domain="regtech.fsec.or.kr")

            logger.info(f"ğŸ“Š Step 2 ì‘ë‹µ: Status={status_code}, Location={location}, JWT={jwt_cookie is not None}")

            if status_code == 302 and (jwt_cookie or "/main/main" in location):
                self.authenticated = True
                logger.info("âœ… REGTECH ì¸ì¦ ì„±ê³µ")
                logger.info(f"ğŸª ì´ ì„¸ì…˜ ì¿ í‚¤: {len(self.session.cookies)}ê°œ")
                self._auth_cache[auth_key] = (time.time(), True)
                self._jwt_expiry = time.time() + 3600  # 1ì‹œê°„ ìœ íš¨ (REGTECH ê¸°ë³¸)
                self.auth_rate_limiter.on_success()
                return True
            else:
                logger.warning(f"âš ï¸ ì¸ì¦ ì‹¤íŒ¨: {status_code}, Location: {location}")
                self.auth_rate_limiter.on_failure(error_code=status_code if status_code >= 400 else None)

        except Exception as e:
            logger.error(f"âŒ ì¸ì¦ ì˜¤ë¥˜: {e}")
            self.auth_rate_limiter.on_failure()

        self._auth_cache[auth_key] = (time.time(), False)
        logger.error("âŒ REGTECH ì¸ì¦ ì‹¤íŒ¨")
        return False

    def _is_jwt_valid(self) -> bool:
        """Check if current JWT token is still valid (5ë¶„ ì—¬ìœ )"""
        if not self._jwt_expiry:
            return False
        # 5ë¶„ ì—¬ìœ ë‘ê³  ë§Œë£Œ ì²´í¬
        return time.time() < (self._jwt_expiry - 300)

    def _ensure_authenticated(self) -> bool:
        """Ensure valid authentication, re-auth if needed"""
        if self.authenticated and self._is_jwt_valid():
            return True
        if self._last_credentials:
            logger.info("ğŸ”„ ì„¸ì…˜ ë§Œë£Œ - ìë™ ì¬ì¸ì¦ ì‹œë„")
            return self.authenticate(*self._last_credentials)
        logger.warning("âš ï¸ ì €ì¥ëœ ì¸ì¦ ì •ë³´ ì—†ìŒ - ì¬ì¸ì¦ í•„ìš”")
        return False

    def collect_blacklist_data(
        self,
        page_size: int = 2000,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        max_pages: int = 100,
    ) -> List[Dict[str, Any]]:
        """ìŠ¤ë§ˆíŠ¸ ë‹¤ë‹¨ê³„ ë‚ ì§œ ë²”ìœ„ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘ - Excel ìš°ì„ """
        if not self._ensure_authenticated():
            logger.error("âŒ ì¸ì¦ë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ìˆ˜ì§‘ ì‹œë„")
            return []

        collection_start = time.time()

        # Excel ìˆ˜ì§‘ ë¹„í™œì„±í™” ì˜µì…˜ (í™˜ê²½ë³€ìˆ˜)
        disable_excel = os.getenv("DISABLE_EXCEL_COLLECTION", "false").lower() == "true"

        if start_date and end_date and not disable_excel:
            logger.info(f"ğŸ“¥ Excel ë‹¤ìš´ë¡œë“œ ì‹œë„: {start_date} ~ {end_date}")
            excel_data = self._download_excel_data(start_date, end_date)

            # Excel ë°ì´í„° ìµœì‹ ì„± ê²€ì¦
            is_excel_stale = False
            if excel_data:
                # ë°ì´í„°ì˜ ìµœëŒ€ íƒì§€ì¼ í™•ì¸ (None ì œì™¸, ë¬¸ìì—´ë§Œ)
                valid_dates = []
                for item in excel_data:
                    d = item.get("detection_date")
                    if d and isinstance(d, str):
                        valid_dates.append(d)

                max_date_str = max(valid_dates) if valid_dates else None

                if max_date_str:
                    try:
                        max_date = datetime.strptime(max_date_str, "%Y-%m-%d").date()
                        today = datetime.now().date()
                        # ë°ì´í„°ê°€ 3ì¼ ì´ìƒ ì˜¤ë˜ëœ ê²½ìš° Staleë¡œ íŒë‹¨
                        if (today - max_date).days > 3:
                            is_excel_stale = True
                            logger.warning(f"âš ï¸ Excel ë°ì´í„°ê°€ ì˜¤ë˜ë¨ (ìµœì‹ : {max_date_str}). HTML ìˆ˜ì§‘ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    except Exception:
                        pass

            if excel_data and not is_excel_stale:
                collection_time = time.time() - collection_start
                logger.info(f"âœ… Excel ìˆ˜ì§‘ ì™„ë£Œ: {len(excel_data)}ê°œ IP ({collection_time:.2f}ì´ˆ)")
                return self._post_process_collected_data(excel_data)

            if not is_excel_stale:
                logger.warning("âš ï¸ Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, HTML í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì „í™˜")

        collected_data = []
        date_strategies = self._generate_date_strategies(start_date, end_date)

        try:
            logger.info(f"ğŸš€ ìŠ¤ë§ˆíŠ¸ REGTECH ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (í˜ì´ì§€ í¬ê¸°: {page_size}, ìµœëŒ€ í˜ì´ì§€: {max_pages})")
            logger.info(f"ğŸ“… ë‚ ì§œ ì „ëµ ìˆ˜: {len(date_strategies)}ê°œ")

            for strategy_idx, (strategy_name, start_dt, end_dt) in enumerate(date_strategies, 1):
                logger.info(f"ğŸ”„ ì „ëµ {strategy_idx}/{len(date_strategies)}: {strategy_name} ({start_dt} ~ {end_dt})")

                if start_dt and end_dt:
                    excel_data = self._download_excel_data(start_dt, end_dt)
                    if excel_data:
                        logger.info(f"âœ… ì „ëµ {strategy_name} Excel ì„±ê³µ: {len(excel_data)}ê°œ IP")
                        collected_data.extend(excel_data)
                        break

                strategy_data = []
                for page_num in range(1, max_pages + 1):
                    page_data = self._collect_single_page(page_num, page_size, start_dt, end_dt)

                    if not page_data:
                        logger.info(f"ğŸ“„ ì „ëµ {strategy_name} í˜ì´ì§€ {page_num}: ë°ì´í„° ì—†ìŒ")
                        break

                    strategy_data.extend(page_data)
                    logger.info(f"ğŸ“„ ì „ëµ {strategy_name} í˜ì´ì§€ {page_num}: {len(page_data)}ê°œ IP ìˆ˜ì§‘")

                    if len(strategy_data) >= 10000000:
                        logger.warning("âš ï¸ ë©”ëª¨ë¦¬ í•œê³„ ë„ë‹¬ (1000ë§Œê°œ), í˜„ì¬ ì „ëµ ì¤‘ë‹¨")
                        break

                if strategy_data:
                    logger.info(f"âœ… ì „ëµ {strategy_name} ì„±ê³µ: {len(strategy_data)}ê°œ IP ìˆ˜ì§‘")
                    collected_data.extend(strategy_data)
                    break
                else:
                    logger.warning(f"âš ï¸ ì „ëµ {strategy_name} ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ")

            collection_time = time.time() - collection_start
            logger.info(f"âœ… ìŠ¤ë§ˆíŠ¸ REGTECH ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data)}ê°œ IP ({collection_time:.2f}ì´ˆ)")

            self._record_collection_performance(collected_data, date_strategies, collection_time)

            return self._post_process_collected_data(collected_data)

        except Exception as e:
            logger.error(f"âŒ REGTECH ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return collected_data

    def _generate_date_strategies(
        self, start_date: Optional[str] = None, end_date: Optional[str] = None
    ) -> List[tuple]:
        """ìŠ¤ë§ˆíŠ¸ ë‚ ì§œ ë²”ìœ„ ì „ëµ ìƒì„± - ë°ì´í„° ê°€ìš©ì„± ìµœì í™”"""
        strategies = []

        # ë‚ ì§œ í•„í„° ì—†ì´ ì „ì²´ ìˆ˜ì§‘ ëª¨ë“œ (REGTECH ì›¹ì‚¬ì´íŠ¸ëŠ” ë‚ ì§œ ì—†ì„ ë•Œ ì „ì²´ ë°ì´í„° ë°˜í™˜)
        if start_date is None and end_date is None:
            strategies.append(("ì „ì²´ ë°ì´í„°", None, None))
            logger.info("ğŸ“‹ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“œ í™œì„±í™” (ë‚ ì§œ í•„í„° ì—†ìŒ)")
            return strategies

        today = datetime.now()

        # ê¸°ë³¸ ë‚ ì§œ ì„¤ì •
        if not end_date:
            end_date = today.strftime("%Y-%m-%d")

        # ì „ëµ 1: ìµœê·¼ 1ì¼ (ì¼ì¼ ë°ì´í„° - ì •ê¸° ìˆ˜ì§‘ìš©)
        recent_start = (today - timedelta(days=1)).strftime("%Y-%m-%d")
        strategies.append(("ìµœê·¼ 1ì¼ ì¼ì¼", recent_start, end_date))

        # ì „ëµ 2: ìµœê·¼ 90ì¼ (ë¶„ê¸° ë°ì´í„° - ì´ˆê¸° ìˆ˜ì§‘ìš©)
        quarter_start = (today - timedelta(days=90)).strftime("%Y-%m-%d")
        strategies.append(("ìµœê·¼ 90ì¼ ë¶„ê¸°", quarter_start, end_date))

        # ì „ëµ 3: ì‚¬ìš©ì ì§€ì • ë²”ìœ„ (ìˆëŠ” ê²½ìš°)
        if start_date:
            strategies.insert(0, ("ì‚¬ìš©ì ì§€ì •", start_date, end_date))

        logger.info(f"ğŸ“‹ ìƒì„±ëœ ë‚ ì§œ ì „ëµ: {[s[0] for s in strategies]}")
        return strategies

    def _record_collection_performance(self, collected_data: List[Dict], strategies: List[tuple], duration: float):
        """ìˆ˜ì§‘ ì„±ê³¼ ê¸°ë¡ ë° ë¶„ì„"""
        performance_log = {
            "timestamp": datetime.now().isoformat(),
            "total_strategies": len(strategies),
            "successful_collection": len(collected_data) > 0,
            "data_count": len(collected_data),
            "duration_seconds": round(duration, 2),
            "strategies_tried": [s[0] for s in strategies],
            "success_rate": (1 if len(collected_data) > 0 else 0) / len(strategies),
        }

        logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì„±ê³¼: {performance_log}")

        # ì„±ê³¼ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥ (í–¥í›„ ìµœì í™”ìš©)
        self._performance_cache = getattr(self, "_performance_cache", [])
        self._performance_cache.append(performance_log)

        # ìµœê·¼ 10ê°œ ì„±ê³¼ë§Œ ìœ ì§€
        if len(self._performance_cache) > 10:
            self._performance_cache = self._performance_cache[-10:]

    def _download_excel_data(
        self,
        start_date: str,
        end_date: str,
    ) -> List[Dict[str, Any]]:
        """Excel ë‹¤ìš´ë¡œë“œë¥¼ í†µí•œ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (ìµœëŒ€ 3ê°œì›”)

        Delegates to regtech_excel.download_excel_data()
        """
        return download_excel_data(
            session=self.session,
            rate_limiter=self.rate_limiter,
            proxy_url=self.proxy_url,
            base_url=self.base_url,
            start_date=start_date,
            end_date=end_date,
        )

    def _collect_single_page(
        self,
        page_num: int,
        page_size: int,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """HAR ë¶„ì„ ê¸°ë°˜ ë‹¨ì¼ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ - ì‹¤ì œ í”„ë¡œí† ì½œ êµ¬í˜„"""
        try:
            # HAR ë¶„ì„ì—ì„œ í™•ì¸ëœ ì‹¤ì œ ì—”ë“œí¬ì¸íŠ¸
            # tabSortë¥¼ URL íŒŒë¼ë¯¸í„°ë¡œë„ ì¶”ê°€í•˜ì—¬ ì„œë²„ê°€ í™•ì‹¤íˆ ì¸ì‹í•˜ë„ë¡ í•¨
            data_url = f"{self.base_url}/fcti/securityAdvisory/advisoryList?tabSort=blacklist"

            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = f"page_{page_num}_{page_size}_{start_date}_{end_date}_blacklist"
            if cache_key in self._data_cache:
                cache_time, cached_data = self._data_cache[cache_key]
                if time.time() - cache_time < 60:  # 1ë¶„ ìºì‹œ
                    logger.info(f"ğŸ“¦ í˜ì´ì§€ {page_num} ìºì‹œ ì‚¬ìš©")
                    return cached_data

            # UIì— ë§ì¶° íŒŒë¼ë¯¸í„° ì¡°ì •
            request_data = {
                "page": str(page_num - 1),
                "tabSort": "blacklist",
                "excelDownload": "",
                "cveId": "",
                "ipId": "",
                "estId": "",
                "startDate": start_date or "",
                "endDate": end_date or "",
                "findCondition": "all",
                "findKeyword": "",
                "excelDown": "blacklist",
                "size": "50",
            }

            # HAR ë¶„ì„ì—ì„œ í™•ì¸ëœ ì •í™•í•œ í—¤ë”
            headers = {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Encoding": "gzip, deflate, br, zstd",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "application/x-www-form-urlencoded",
                "Origin": "https://regtech.fsec.or.kr",
                "Pragma": "no-cache",
                "Referer": "https://regtech.fsec.or.kr/fcti/securityAdvisory/advisoryList",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "same-origin",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
            }

            logger.info(f"ğŸ”„ HAR ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘: í˜ì´ì§€ {page_num}, í¬ê¸° {page_size}")

            # Rate Limiting ì ìš© (API ì°¨ë‹¨ ë°©ì§€)
            if not self.rate_limiter.wait_if_needed():
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page_num} Rate Limiter ëŒ€ê¸° ì‹¤íŒ¨. ìˆ˜ì§‘ ì¤‘ë‹¨.")
                return []

            # ì¿ í‚¤ í™•ì¸ ë¡œê·¸
            cookie_count = len(self.session.cookies)
            has_jwt = any(c.name == "regtech-va" for c in self.session.cookies)
            logger.info(f"ğŸª ìš”ì²­ ì¿ í‚¤ ìƒíƒœ: {cookie_count}ê°œ, JWT ì¡´ì¬: {has_jwt}")

            import subprocess
            import json
            import urllib.parse

            headers_list = []
            for k, v in headers.items():
                headers_list.extend(["-H", f"{k}: {v}"])

            cookie_header = "; ".join([f"{c.name}={c.value}" for c in self.session.cookies])
            if cookie_header:
                headers_list.extend(["-H", f"Cookie: {cookie_header}"])

            # data_json = json.dumps(request_data)

            curl_cmd = (
                ["curl", "-s", "-X", "POST", data_url]
                + headers_list
                + [
                    "-H",
                    "Content-Type: application/x-www-form-urlencoded",
                    "--data",
                    urllib.parse.urlencode(request_data),
                ]
            )

            if self.proxy_url:
                curl_cmd.extend(["--proxy", self.proxy_url])

            encoded_data = urllib.parse.urlencode(request_data)
            logger.info(f"ğŸ” ë°ì´í„° ìˆ˜ì§‘ curl ì‹¤í–‰: {data_url} | page={page_num - 1}")
            logger.debug(f"ğŸ“¤ POST ë°ì´í„°: {encoded_data}")

            result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=60)

            if result.returncode != 0:
                logger.error(f"âŒ curl ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
                self.rate_limiter.on_failure()
                return []

            response_text = result.stdout
            logger.info(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response_text)}")

            try:
                with open("/tmp/regtech_debug.html", "w", encoding="utf-8") as f:
                    f.write(response_text)
                logger.info("ğŸ’¾ ë””ë²„ê·¸ HTML ì €ì¥ ì™„ë£Œ: /tmp/regtech_debug.html")
            except Exception as e:
                logger.warning(f"âš ï¸ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")

            try:

                class MockResponse:
                    def __init__(self, text, status_code):
                        self.text = text
                        self.status_code = status_code

                    def json(self):
                        return json.loads(self.text)

                response = MockResponse(response_text, 200)

            except Exception as e:
                logger.error(f"âŒ ì‘ë‹µ ê°ì²´ ìƒì„± ì‹¤íŒ¨: {e}")
                return []

            logger.info("ğŸ“Š ì‘ë‹µ ì²˜ë¦¬ ì‹œì‘...")

            if response.status_code == 200:
                page_data = self._parse_response_data(response)

                # ê²°ê³¼ ìºì‹œ
                self._data_cache[cache_key] = (time.time(), page_data)

                # Rate Limiter ì„±ê³µ í”¼ë“œë°±
                self.rate_limiter.on_success()

                logger.info(f"âœ… í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì™„ë£Œ: {len(page_data)}ê°œ í•­ëª©")
                return page_data
            else:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page_num} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

                # Rate Limiter ì‹¤íŒ¨ í”¼ë“œë°±
                self.rate_limiter.on_failure(error_code=response.status_code)

                return []

        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

            # Rate Limiter ì‹¤íŒ¨ í”¼ë“œë°±
            self.rate_limiter.on_failure()

            return []

    def _parse_response_data(self, response) -> List[Dict[str, Any]]:
        """ì‘ë‹µ ë°ì´í„° íŒŒì‹± - ìµœì í™”ëœ ì²˜ë¦¬"""
        try:
            # JSON íŒŒì‹± ì‹œë„
            json_data = response.json()

            if isinstance(json_data, dict) and "data" in json_data:
                raw_data = json_data["data"]
            elif isinstance(json_data, list):
                raw_data = json_data
            else:
                logger.warning("âš ï¸ ì˜ˆìƒí•˜ì§€ ëª»í•œ JSON ì‘ë‹µ í˜•ì‹")
                return self._parse_html_response(response.text)

            # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
            processed_data = []
            for item in raw_data:
                processed_item = self._process_regtech_item(item)
                if processed_item:
                    processed_data.append(processed_item)

            return processed_data

        except json.JSONDecodeError:
            logger.info("ğŸ“„ JSON íŒŒì‹± ì‹¤íŒ¨, HTML íŒŒì‹±ìœ¼ë¡œ ì „í™˜")
            return self._parse_html_response(response.text)
        except Exception as e:
            logger.error(f"âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []

    def _post_process_collected_data(self, collected_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ìˆ˜ì§‘ëœ ë°ì´í„° í›„ì²˜ë¦¬ - ìµœì í™” ë° ì •ì œ"""
        if not collected_data:
            return []

        logger.info(f"ğŸ”§ ìˆ˜ì§‘ ë°ì´í„° í›„ì²˜ë¦¬ ì‹œì‘: {len(collected_data)}ê°œ í•­ëª©")

        # 1. IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ì œ
        valid_data = []
        for item in collected_data:
            ip_addr = item.get("ip_address", "").strip()
            if self._is_valid_ip(ip_addr):
                # IP ì •ê·œí™”
                item["ip_address"] = ip_addr
                valid_data.append(item)

        # 2. ì¤‘ë³µ ì œê±° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        unique_data = self._fast_deduplication(valid_data)

        # 3. ë°ì´í„° í’ˆì§ˆ í–¥ìƒ
        enhanced_data = []
        for item in unique_data:
            enhanced_item = self._enhance_data_quality(item)
            enhanced_data.append(enhanced_item)

        logger.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ: {len(enhanced_data)}ê°œ ê³ í’ˆì§ˆ IP")
        return enhanced_data

    def _fast_deduplication(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê³ ì† ì¤‘ë³µ ì œê±°"""
        seen_ips = set()
        unique_data = []

        for item in data:
            ip_key = f"{item.get('ip_address')}:{item.get('source', 'REGTECH')}"
            if ip_key not in seen_ips:
                seen_ips.add(ip_key)
                unique_data.append(item)

        return unique_data

    def _enhance_data_quality(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í–¥ìƒ - í•´ì œì¼ ê¸°ì¤€ í™œì„± ìƒíƒœ ë¡œì§ ì ìš©"""

        # í•´ì œì¼ ì¶”ì¶œ
        removal_date = self._parse_date(item.get("removal_date"))

        # í™œì„± ìƒíƒœ ê²°ì • (removal_date ê¸°ë°˜)
        is_active = True
        if removal_date:
            # í•´ì œì¼ì´ ì˜¤ëŠ˜ë³´ë‹¤ ì´ì „ì´ë©´ ë¹„í™œì„±í™”
            today = datetime.now().date()
            if isinstance(removal_date, str):
                removal_date_obj = datetime.strptime(removal_date, "%Y-%m-%d").date()
            else:
                removal_date_obj = removal_date.date() if hasattr(removal_date, "date") else removal_date

            if removal_date_obj < today:
                is_active = False
                logger.info(
                    f"ğŸ”´ [í’ˆì§ˆí–¥ìƒ] IP {item.get('ip_address')} ë¹„í™œì„±í™”: í•´ì œì¼ {removal_date_obj} < ì˜¤ëŠ˜ {today}"
                )

        # ê¸°ë³¸ê°’ ì„¤ì • ë° ë°ì´í„° ì •ì œ - ì›ë³¸ reason ìš°ì„  ë³´ì¡´
        original_reason = item.get("reason", "")
        if not original_reason or original_reason in [
            "REGTECH HTML Parse",
            "REGTECH Blacklist",
        ]:
            # ì¶”ê°€ í•„ë“œì—ì„œ ì˜ë¯¸ ìˆëŠ” ë‚´ìš© ì°¾ê¸°
            for alt_field in [
                "contents",
                "detail",
                "description",
                "threat_desc",
                "block_reason",
            ]:
                alt_content = item.get(alt_field, "").strip()
                if alt_content and len(alt_content) > 5:  # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´
                    original_reason = alt_content
                    break
            if not original_reason:
                original_reason = "REGTECH ìœ„í˜‘ IP"

        enhanced_item = {
            "ip_address": item.get("ip_address", "").strip(),
            "source": "REGTECH",
            "reason": original_reason,
            "confidence_level": self._determine_confidence(item),
            "detection_count": 1,
            "is_active": is_active,  # í•´ì œì¼ ê¸°ì¤€ ë¡œì§ ì ìš©
            "last_seen": datetime.now(),
            "country": self._normalize_country_code(item.get("country")),
            "detection_date": self._parse_date(item.get("detection_date")),
            "removal_date": removal_date,
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            "collection_timestamp": datetime.now().isoformat(),
            "data_source_version": "optimized_v2.0",
        }

        return enhanced_item

    def _normalize_country_code(self, country_value) -> Optional[str]:
        """êµ­ê°€ ì½”ë“œ ì •ê·œí™”"""
        if not country_value:
            return None

        country_str = str(country_value).upper().strip()

        # ì¼ë°˜ì ì¸ êµ­ê°€ ì½”ë“œ ë§¤í•‘
        country_mapping = {
            "KR": "KR",
            "KOREA": "KR",
            "í•œêµ­": "KR",
            "US": "US",
            "USA": "US",
            "UNITED STATES": "US",
            "CN": "CN",
            "CHINA": "CN",
            "ì¤‘êµ­": "CN",
            "JP": "JP",
            "JAPAN": "JP",
            "ì¼ë³¸": "JP",
        }

        return country_mapping.get(country_str, country_str[:2] if len(country_str) >= 2 else None)

    def _process_regtech_item(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """REGTECH ë°ì´í„° í•­ëª© ì²˜ë¦¬ - ìµœì í™”"""
        try:
            # IP ì£¼ì†Œ ì¶”ì¶œ - ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì›
            ip_fields = ["ipAddr", "ip_address", "ip", "IP", "target_ip"]
            ip_address = None

            for field in ip_fields:
                if field in item and item[field]:
                    ip_address = str(item[field]).strip()
                    break

            if not ip_address or not self._is_valid_ip(ip_address):
                return None

            # ì›ë³¸ ë°ì´í„° ë¡œê¹… ë° ê²€ì¦
            logger.info(f"ğŸ” ì›ë³¸ REGTECH ë°ì´í„°: {item}")

            # íƒì§€ì¼/í•´ì œì¼ í•„ë“œ í™•ì¸
            detection_fields = [
                "regDt",
                "detectionDate",
                "reg_dt",
                "detect_dt",
                "created_dt",
            ]
            removal_fields = ["delDt", "removalDate", "del_dt", "remove_dt", "end_dt"]
            reason_fields = [
                "blockReason",
                "reason",
                "block_reason",
                "description",
                "content",
            ]

            detection_date = None
            removal_date = None
            detection_reason = "REGTECH Blacklist"

            # íƒì§€ì¼ ì¶”ì¶œ
            for field in detection_fields:
                if field in item and item[field]:
                    detection_date = self._parse_date(item[field])
                    logger.info(f"âœ… íƒì§€ì¼ ë°œê²¬: {field} = {item[field]} -> {detection_date}")
                    break

            # í•´ì œì¼ ì¶”ì¶œ
            for field in removal_fields:
                if field in item and item[field]:
                    removal_date = self._parse_date(item[field])
                    logger.info(f"âœ… í•´ì œì¼ ë°œê²¬: {field} = {item[field]} -> {removal_date}")
                    break

            # íƒì§€ë‚´ìš© ì¶”ì¶œ - ì›ë³¸ raw data ìš°ì„ 
            for field in reason_fields:
                if field in item and item[field]:
                    raw_reason = str(item[field]).strip()
                    if raw_reason and raw_reason not in [
                        "REGTECH HTML Parse",
                        "REGTECH Blacklist",
                        "",
                    ]:
                        detection_reason = raw_reason
                        logger.info(f"âœ… ì›ë³¸ íƒì§€ë‚´ìš© ë°œê²¬: {field} = {detection_reason}")
                        break
                    elif raw_reason:
                        # ê¸°ë³¸ê°’ë³´ë‹¤ëŠ” ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì´ë©´ ì‚¬ìš©
                        detection_reason = raw_reason
                        logger.info(f"ğŸ“ ê¸°ë³¸ íƒì§€ë‚´ìš© ì‚¬ìš©: {field} = {detection_reason}")

            # ì¶”ê°€ í•„ë“œì—ì„œ ë” ìì„¸í•œ ë‚´ìš© ì°¾ê¸°
            additional_content_fields = [
                "contents",
                "detail",
                "description",
                "threat_desc",
                "attack_info",
                "malware_name",
            ]
            for field in additional_content_fields:
                if field in item and item[field]:
                    additional_content = str(item[field]).strip()
                    if additional_content and len(additional_content) > len(detection_reason):
                        detection_reason = additional_content
                        logger.info(f"âœ… ìƒì„¸ íƒì§€ë‚´ìš© ë°œê²¬: {field} = {detection_reason}")
                        break

            # í™œì„± ìƒíƒœ ê²°ì • (removal_date ê¸°ë°˜)
            is_active = True
            if removal_date:
                # í•´ì œì¼ì´ ì˜¤ëŠ˜ë³´ë‹¤ ì´ì „ì´ë©´ ë¹„í™œì„±í™” (ì˜¤ëŠ˜ í•´ì œë˜ëŠ” IPëŠ” í™œì„± ìœ ì§€)
                today = datetime.now().date()
                if isinstance(removal_date, str):
                    removal_date_obj = datetime.strptime(removal_date, "%Y-%m-%d").date()
                else:
                    removal_date_obj = removal_date.date() if hasattr(removal_date, "date") else removal_date

                if removal_date_obj < today:
                    is_active = False
                    logger.info(f"ğŸ”´ IP {ip_address} ë¹„í™œì„±í™”: í•´ì œì¼ {removal_date_obj} < ì˜¤ëŠ˜ {today}")

            processed_item = {
                "ip_address": ip_address,
                "source": "REGTECH",
                "reason": detection_reason,
                "confidence_level": self._determine_confidence(item),
                "detection_count": 1,
                "is_active": is_active,
                "last_seen": datetime.now(),
                "country": item.get("country") or item.get("countryCode"),
                "detection_date": detection_date,
                "removal_date": removal_date,
                "raw_data": {
                    "api_response": item,
                    "collection_timestamp": datetime.now().isoformat(),
                },
            }

            return processed_item

        except Exception as e:
            logger.debug(f"í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            return None

    def _determine_confidence(self, item: Dict[str, Any]) -> int:
        """ì‹ ë¢°ë„ ê²°ì • - í–¥ìƒëœ í‰ê°€"""
        base_confidence = 80  # REGTECH ê¸°ë³¸ ì‹ ë¢°ë„

        # ìœ„í˜‘ ë ˆë²¨ ê¸°ë°˜ ì¡°ì •
        threat_level = str(item.get("threatLevel", "medium")).lower()
        threat_adjustments = {"critical": 15, "high": 10, "medium": 0, "low": -10}

        confidence = base_confidence + threat_adjustments.get(threat_level, 0)

        # ì¶”ê°€ ì‹ ë¢°ë„ ìš”ì†Œ
        if item.get("verified"):
            confidence += 5
        if item.get("reportCount", 0) > 10:
            confidence += 5

        return max(10, min(100, confidence))

    def _parse_date(self, date_str: Any) -> Optional[str]:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± - í–¥ìƒëœ ì²˜ë¦¬ ë° ë¡œê¹…"""
        if not date_str:
            return None

        # ì›ë³¸ ë°ì´í„° ë¡œê¹…
        logger.info(f"ğŸ“… ë‚ ì§œ íŒŒì‹± ì‹œë„: '{date_str}' (íƒ€ì…: {type(date_str)})")

        # ë¬¸ìì—´ë¡œ ë³€í™˜
        # date_str_clean = str(date_str).strip()

        date_formats = [
            "%Y-%m-%d",
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d",
            "%Y.%m.%d",
            "%d-%m-%Y",
            "%d/%m/%Y",
            "%d.%m.%Y",
            "%Y%m%d",  # 8ìë¦¬ ìˆ«ì
            "%m/%d/%Y",
            "%m-%d-%Y",
        ]

        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                result = parsed_date.strftime("%Y-%m-%d")
                logger.info(f"âœ… ë‚ ì§œ íŒŒì‹± ì„±ê³µ: '{date_str}' -> '{result}' (í˜•ì‹: {fmt})")
                return result
            except ValueError:
                continue

        logger.warning(f"âŒ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: '{date_str}' - ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹")
        return None

    def _parse_html_response(self, html_content: str) -> List[Dict[str, Any]]:
        return parse_html_response(html_content)

    def _extract_country_info(self, cell_texts: List[str]) -> Optional[str]:
        """HTML í…Œì´ë¸” í–‰ì—ì„œ êµ­ê°€ ì •ë³´ ì¶”ì¶œ"""
        if not cell_texts:
            return None

        # êµ­ê°€ ì½”ë“œ/ì´ë¦„ì„ ì°¾ê¸° ìœ„í•œ íŒ¨í„´ë“¤
        country_patterns = {
            "KR": ["KR", "Korea", "í•œêµ­", "South Korea", "Republic of Korea"],
            "US": ["US", "USA", "United States", "ë¯¸êµ­", "America"],
            "CN": ["CN", "China", "ì¤‘êµ­", "CHN"],
            "JP": ["JP", "Japan", "ì¼ë³¸", "JPN"],
            "RU": ["RU", "Russia", "ëŸ¬ì‹œì•„", "Russian"],
            "DE": ["DE", "Germany", "ë…ì¼", "German"],
            "FR": ["FR", "France", "í”„ë‘ìŠ¤", "French"],
            "GB": ["GB", "UK", "United Kingdom", "ì˜êµ­", "Britain"],
            "IN": ["IN", "India", "ì¸ë„", "Indian"],
        }

        # ê° ì…€ì—ì„œ êµ­ê°€ ì •ë³´ ì°¾ê¸°
        for cell_text in cell_texts:
            if not cell_text or len(cell_text.strip()) < 2:
                continue

            cell_upper = cell_text.upper().strip()

            # êµ­ê°€ ì½”ë“œ ë§¤ì¹­
            for country_code, patterns in country_patterns.items():
                for pattern in patterns:
                    if pattern.upper() in cell_upper:
                        logger.info(f"âœ… êµ­ê°€ ì •ë³´ ë°œê²¬: '{cell_text}' -> {country_code}")
                        return country_code

            # 2ê¸€ì êµ­ê°€ ì½”ë“œ í˜•íƒœì¸ì§€ í™•ì¸
            if len(cell_text.strip()) == 2 and cell_text.strip().isalpha():
                country_code = cell_text.strip().upper()
                logger.info(f"âœ… êµ­ê°€ ì½”ë“œ ë°œê²¬: {country_code}")
                return country_code

        return None

    def _is_valid_ip(self, ip_str: str) -> bool:
        """IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì‚¬ - í–¥ìƒëœ ê²€ì¦"""
        try:
            import ipaddress

            ip_obj = ipaddress.ip_address(ip_str.strip())

            # ì‚¬ì„¤ IP ë° íŠ¹ìˆ˜ IP í•„í„°ë§
            if ip_obj.is_private or ip_obj.is_loopback or ip_obj.is_multicast:
                return False

            # IPv4 ìš°ì„ , IPv6 ì§€ì›
            return True

        except ValueError:
            return False

    def get_session_info(self) -> Dict[str, Any]:
        """ì„¸ì…˜ ì •ë³´ ë°˜í™˜ - í–¥ìƒëœ ì •ë³´"""
        return {
            "authenticated": self.authenticated,
            "cookies_count": len(self.session.cookies),
            "base_url": self.base_url,
            "cache_size": len(self._data_cache),
            "auth_cache_size": len(self._auth_cache),
            "performance_mode": "optimized",
            "last_activity": datetime.now().isoformat(),
            "rate_limiter": self.rate_limiter.get_stats(),
        }

    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self._data_cache.clear()
        self._auth_cache.clear()
        logger.info("ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
regtech_collector = RegtechCollector()
