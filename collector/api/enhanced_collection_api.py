"""
Enhanced Collection API
ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ì„ ìœ„í•œ í™•ì¥ëœ ìˆ˜ì§‘ API
"""

import asyncio
from flask import Blueprint, jsonify, request
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List
import time

from collector.core.multi_source_collector import multi_source_collector, SourceType
from collector.config import CollectorConfig

logger = logging.getLogger(__name__)

# ë¸”ë£¨í”„ë¦°íŠ¸ ìƒì„±
enhanced_api = Blueprint("enhanced_collection", __name__)


@enhanced_api.route("/multi-source/collect", methods=["POST"])
def multi_source_collection():
    """ë‹¤ì¤‘ ì†ŒìŠ¤ ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ API"""
    try:
        # ìš”ì²­ íŒŒë¼ë¯¸í„°
        params = request.get_json() or {}

        max_ips_per_source = params.get("max_ips_per_source", 50000)
        parallel_sources = params.get("parallel_sources", 5)
        date_range_days = params.get("date_range_days", 7)
        enabled_sources = params.get("enabled_sources", [])  # íŠ¹ì • ì†ŒìŠ¤ë§Œ í™œì„±í™”

        logger.info(
            f"ğŸš€ ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {max_ips_per_source:,}ê°œ/ì†ŒìŠ¤, {parallel_sources}ê°œ ë³‘ë ¬"
        )

        # ì†ŒìŠ¤ í™œì„±í™” ì„¤ì •
        if enabled_sources:
            # ëª¨ë“  ì†ŒìŠ¤ ë¹„í™œì„±í™” í›„ ì„ íƒëœ ì†ŒìŠ¤ë§Œ í™œì„±í™”
            for source_id, config in multi_source_collector.sources.items():
                config.enabled = any(
                    source_type in source_id for source_type in enabled_sources
                )

        # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‹¤í–‰ (ë™ê¸° ë˜í¼)
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            collection_result = loop.run_until_complete(
                multi_source_collector.collect_from_all_sources(
                    max_ips_per_source=max_ips_per_source,
                    parallel_sources=parallel_sources,
                    date_range_days=date_range_days,
                )
            )
        finally:
            loop.close()

        # ê²°ê³¼ ì²˜ë¦¬
        if collection_result.get("success"):
            collected_data = collection_result.get("data", [])

            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (ê¸°ì¡´ collectorì™€ í†µí•©)
            saved_count = 0
            try:
                from collector.database import CollectorDatabase

                db = CollectorDatabase()

                for item in collected_data:
                    if db.save_blacklist_ip(item):
                        saved_count += 1

                logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {saved_count}/{len(collected_data)}ê°œ")

            except Exception as db_error:
                logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {db_error}")

            # ì„±ê³µ ì‘ë‹µ
            return jsonify(
                {
                    "success": True,
                    "message": f"ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {collection_result['total_collected']:,}ê°œ IP",
                    "total_collected": collection_result["total_collected"],
                    "unique_ips": collection_result["unique_ips"],
                    "sources_successful": collection_result["sources_successful"],
                    "sources_attempted": collection_result["sources_attempted"],
                    "collection_time": collection_result["collection_time_seconds"],
                    "saved_to_db": saved_count,
                    "source_breakdown": collection_result["source_breakdown"],
                    "timestamp": collection_result["timestamp"],
                }
            )
        else:
            return (
                jsonify(
                    {
                        "success": False,
                        "error": "ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨",
                        "details": collection_result,
                    }
                ),
                500,
            )

    except Exception as e:
        logger.error(f"âŒ ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ API ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": f"ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500


@enhanced_api.route("/sources/status", methods=["GET"])
def get_sources_status():
    """ìœ„í˜‘ ì •ë³´ ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    try:
        status = multi_source_collector.get_source_status()
        return jsonify(
            {"success": True, "data": status, "timestamp": datetime.now().isoformat()}
        )
    except Exception as e:
        logger.error(f"âŒ ì†ŒìŠ¤ ìƒíƒœ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@enhanced_api.route("/sources/configure", methods=["POST"])
def configure_sources():
    """ì†ŒìŠ¤ ì„¤ì • ë³€ê²½"""
    try:
        config_data = request.get_json()

        # ì†ŒìŠ¤ í™œì„±í™”/ë¹„í™œì„±í™”
        if "enable_sources" in config_data:
            for source_type in config_data["enable_sources"]:
                multi_source_collector.enable_source(source_type, True)

        if "disable_sources" in config_data:
            for source_type in config_data["disable_sources"]:
                multi_source_collector.enable_source(source_type, False)

        # ì‚¬ìš©ì ì •ì˜ ì†ŒìŠ¤ ì¶”ê°€
        if "add_custom_source" in config_data:
            custom = config_data["add_custom_source"]
            success = multi_source_collector.add_custom_source(
                name=custom["name"],
                url=custom["url"],
                source_type=SourceType.CUSTOM_API,
                api_key=custom.get("api_key"),
                headers=custom.get("headers"),
                confidence_boost=custom.get("confidence_boost", 0),
            )

            if not success:
                return jsonify({"success": False, "error": "ì‚¬ìš©ì ì •ì˜ ì†ŒìŠ¤ ì¶”ê°€ ì‹¤íŒ¨"}), 400

        return jsonify(
            {
                "success": True,
                "message": "ì†ŒìŠ¤ ì„¤ì • ë³€ê²½ ì™„ë£Œ",
                "new_status": multi_source_collector.get_source_status(),
            }
        )

    except Exception as e:
        logger.error(f"âŒ ì†ŒìŠ¤ ì„¤ì • ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@enhanced_api.route("/collection/wide-range", methods=["POST"])
def wide_range_collection():
    """ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ - ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œ"""
    try:
        params = request.get_json() or {}

        # ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ ì„¤ì •
        max_total_ips = params.get("max_total_ips", 500000)  # ì´ 50ë§Œê°œ
        aggressive_mode = params.get("aggressive_mode", False)
        date_range_days = params.get("date_range_days", 30)  # 30ì¼ ë²”ìœ„

        logger.info(f"ğŸŒ ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ ì‹œì‘: ìµœëŒ€ {max_total_ips:,}ê°œ IP")

        collection_start = time.time()

        # 1ë‹¨ê³„: ëª¨ë“  ê³ ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ìˆ˜ì§‘
        high_priority_sources = ["regtech", "threatfox", "feodo", "urlhaus"]

        # 2ë‹¨ê³„: ì¤‘ê°„ ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ìˆ˜ì§‘
        medium_priority_sources = ["phishtank", "openphish"]

        total_collected = []
        stage_results = {}

        # 1ë‹¨ê³„ ì‹¤í–‰
        logger.info("ğŸ¯ 1ë‹¨ê³„: ê³ ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ìˆ˜ì§‘")
        stage1_result = _execute_collection_stage(
            high_priority_sources,
            max_total_ips // 2,  # ì ˆë°˜ì„ 1ë‹¨ê³„ì— í• ë‹¹
            8,  # ë³‘ë ¬ ì²˜ë¦¬
            date_range_days,
        )
        total_collected.extend(stage1_result["data"])
        stage_results["stage1"] = stage1_result

        # 2ë‹¨ê³„ ì‹¤í–‰ (ë‚¨ì€ í• ë‹¹ëŸ‰ì´ ìˆëŠ” ê²½ìš°)
        remaining_quota = max_total_ips - len(total_collected)
        if remaining_quota > 0:
            logger.info("ğŸ¯ 2ë‹¨ê³„: ì¤‘ê°„ ìš°ì„ ìˆœìœ„ ì†ŒìŠ¤ ìˆ˜ì§‘")
            stage2_result = _execute_collection_stage(
                medium_priority_sources, remaining_quota, 4, date_range_days  # ë³‘ë ¬ ì²˜ë¦¬
            )
            total_collected.extend(stage2_result["data"])
            stage_results["stage2"] = stage2_result

        # ì¤‘ë³µ ì œê±° ë° ìµœì¢… ì²˜ë¦¬
        final_data = multi_source_collector._deduplicate_and_enhance(total_collected)

        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        saved_count = 0
        try:
            from collector.database import CollectorDatabase

            db = CollectorDatabase()

            # ë°°ì¹˜ ì €ì¥ ìµœì í™”
            batch_size = 1000
            for i in range(0, len(final_data), batch_size):
                batch = final_data[i : i + batch_size]
                for item in batch:
                    if db.save_blacklist_ip(item):
                        saved_count += 1

                if i + batch_size < len(final_data):
                    logger.info(f"ğŸ’¾ ë°°ì¹˜ ì €ì¥ ì§„í–‰: {i+batch_size}/{len(final_data)}")

        except Exception as db_error:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {db_error}")

        collection_time = time.time() - collection_start

        # ì§€ë¦¬ì  ë¶„í¬ ë¶„ì„
        geo_distribution = _analyze_geographical_distribution(final_data)

        # ìœ„í˜‘ ì¹´í…Œê³ ë¦¬ ë¶„í¬
        category_distribution = _analyze_category_distribution(final_data)

        return jsonify(
            {
                "success": True,
                "message": f"ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ ì™„ë£Œ: {len(final_data):,}ê°œ ê³ ìœ  IP",
                "collection_summary": {
                    "total_unique_ips": len(final_data),
                    "total_raw_collected": len(total_collected),
                    "deduplication_rate": round(
                        (1 - len(final_data) / max(len(total_collected), 1)) * 100, 2
                    ),
                    "collection_time_seconds": round(collection_time, 2),
                    "ips_per_second": round(len(final_data) / collection_time, 2)
                    if collection_time > 0
                    else 0,
                    "saved_to_database": saved_count,
                    "save_success_rate": round(saved_count / len(final_data) * 100, 2)
                    if final_data
                    else 0,
                },
                "stage_results": stage_results,
                "geographical_distribution": geo_distribution,
                "category_distribution": category_distribution,
                "data_quality_metrics": {
                    "multi_source_confirmations": len(
                        [ip for ip in final_data if ip.get("multi_source", False)]
                    ),
                    "average_confidence": round(
                        sum(ip.get("confidence_level", 0) for ip in final_data)
                        / len(final_data),
                        2,
                    )
                    if final_data
                    else 0,
                    "active_threats": len(
                        [ip for ip in final_data if ip.get("is_active", True)]
                    ),
                },
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"âŒ ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": f"ë„“ì€ ë²”ìœ„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}), 500


def _execute_collection_stage(
    source_types: List[str], max_ips: int, parallel_count: int, date_range_days: int
) -> Dict[str, Any]:
    """ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰"""
    try:
        # ì„ íƒëœ ì†ŒìŠ¤ë§Œ í™œì„±í™”
        for source_id, config in multi_source_collector.sources.items():
            config.enabled = any(
                source_type in source_id.lower() for source_type in source_types
            )

        # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                multi_source_collector.collect_from_all_sources(
                    max_ips_per_source=max_ips // len(source_types)
                    if source_types
                    else max_ips,
                    parallel_sources=parallel_count,
                    date_range_days=date_range_days,
                )
            )
            return result
        finally:
            loop.close()

    except Exception as e:
        logger.error(f"âŒ ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        return {"success": False, "error": str(e), "data": []}


def _analyze_geographical_distribution(data: List[Dict[str, Any]]) -> Dict[str, int]:
    """ì§€ë¦¬ì  ë¶„í¬ ë¶„ì„"""
    geo_counts = {}

    for item in data:
        country = item.get("country") or "Unknown"
        geo_counts[country] = geo_counts.get(country, 0) + 1

    # ìƒìœ„ 10ê°œ êµ­ê°€
    sorted_geo = sorted(geo_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    return dict(sorted_geo)


def _analyze_category_distribution(data: List[Dict[str, Any]]) -> Dict[str, int]:
    """ìœ„í˜‘ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¶„ì„"""
    category_counts = {}

    for item in data:
        category = item.get("category", "unknown")
        category_counts[category] = category_counts.get(category, 0) + 1

    return category_counts


@enhanced_api.route("/collection/real-time", methods=["POST"])
def real_time_collection():
    """ì‹¤ì‹œê°„ ìˆ˜ì§‘ ëª¨ë“œ"""
    try:
        params = request.get_json() or {}

        # ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì„¤ì •
        duration_minutes = params.get("duration_minutes", 60)  # 1ì‹œê°„
        collection_interval = params.get("interval_seconds", 300)  # 5ë¶„ë§ˆë‹¤

        logger.info(f"â° ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì‹œì‘: {duration_minutes}ë¶„ê°„, {collection_interval}ì´ˆ ê°„ê²©")

        start_time = datetime.now()
        end_time = start_time + timedelta(minutes=duration_minutes)

        real_time_results = []
        total_collected = 0

        # ìŠ¤ì¼€ì¤„ë§ëœ ìˆ˜ì§‘ (ë°ëª¨ìš© - ì‹¤ì œë¡œëŠ” ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ í•„ìš”)
        current_time = datetime.now()

        if current_time < end_time:
            # í˜„ì¬ ì‹œì  ìˆ˜ì§‘ ì‹¤í–‰
            current_result = _execute_collection_stage(
                ["regtech", "threatfox"],  # ì‹¤ì‹œê°„ í”¼ë“œ ì†ŒìŠ¤
                10000,  # ì‹¤ì‹œê°„ ëª¨ë“œëŠ” ì ì€ ìˆ˜ëŸ‰
                3,  # ë‚®ì€ ë³‘ë ¬ì„±
                1,  # ìµœê·¼ 1ì¼
            )

            if current_result.get("success"):
                batch_collected = len(current_result.get("data", []))
                total_collected += batch_collected

                real_time_results.append(
                    {
                        "timestamp": current_time.isoformat(),
                        "collected_count": batch_collected,
                        "sources": current_result.get("sources_successful", 0),
                    }
                )

        return jsonify(
            {
                "success": True,
                "message": f"ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì™„ë£Œ: {total_collected:,}ê°œ IP",
                "real_time_summary": {
                    "duration_minutes": duration_minutes,
                    "total_collected": total_collected,
                    "collection_batches": len(real_time_results),
                    "average_per_batch": round(
                        total_collected / max(len(real_time_results), 1), 2
                    ),
                },
                "batch_results": real_time_results,
                "next_collection": (
                    datetime.now() + timedelta(seconds=collection_interval)
                ).isoformat(),
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": f"ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"}), 500


@enhanced_api.route("/analytics/collection-performance", methods=["GET"])
def collection_performance_analytics():
    """ìˆ˜ì§‘ ì„±ëŠ¥ ë¶„ì„"""
    try:
        # ìˆ˜ì§‘ í†µê³„
        stats = multi_source_collector.collection_stats

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        recent_collections = stats.get("collection_history", [])[-10:]  # ìµœê·¼ 10íšŒ

        if recent_collections:
            avg_collection_time = sum(
                c.get("collection_time", 0) for c in recent_collections
            ) / len(recent_collections)
            avg_ips_collected = sum(
                c.get("total_collected", 0) for c in recent_collections
            ) / len(recent_collections)
            avg_sources_used = sum(
                c.get("sources_used", 0) for c in recent_collections
            ) / len(recent_collections)
        else:
            avg_collection_time = 0
            avg_ips_collected = 0
            avg_sources_used = 0

        # ì†ŒìŠ¤ë³„ ì„±ëŠ¥
        source_performance = {}
        for source_id, config in multi_source_collector.sources.items():
            if config.enabled:
                source_performance[config.name] = {
                    "priority": config.priority,
                    "rate_limit": config.rate_limit,
                    "confidence_boost": config.confidence_boost,
                    "estimated_daily_capacity": int(
                        86400 / (1.0 / config.rate_limit)
                    ),  # ì¼ì¼ ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰
                }

        return jsonify(
            {
                "success": True,
                "performance_metrics": {
                    "average_collection_time_seconds": round(avg_collection_time, 2),
                    "average_ips_per_collection": round(avg_ips_collected, 2),
                    "average_sources_per_collection": round(avg_sources_used, 2),
                    "total_lifetime_collected": stats.get("total_collected", 0),
                    "collection_efficiency": round(
                        avg_ips_collected / max(avg_collection_time, 1), 2
                    ),
                },
                "source_performance": source_performance,
                "recent_collections": recent_collections,
                "recommendations": _generate_performance_recommendations(
                    stats, source_performance
                ),
                "timestamp": datetime.now().isoformat(),
            }
        )

    except Exception as e:
        logger.error(f"âŒ ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


def _generate_performance_recommendations(
    stats: Dict, source_performance: Dict
) -> List[str]:
    """ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    recommendations = []

    # ìˆ˜ì§‘ ì´ë ¥ ë¶„ì„
    recent_collections = stats.get("collection_history", [])
    if len(recent_collections) >= 3:
        recent_times = [c.get("collection_time", 0) for c in recent_collections[-3:]]
        avg_time = sum(recent_times) / len(recent_times)

        if avg_time > 120:  # 2ë¶„ ì´ìƒ
            recommendations.append("ìˆ˜ì§‘ ì‹œê°„ì´ ê¸¸ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì†ŒìŠ¤ë³„ ìµœëŒ€ ìˆ˜ì§‘ëŸ‰ì„ ì¡°ì •í•˜ì„¸ìš”.")

        if avg_time < 30:  # 30ì´ˆ ë¯¸ë§Œ
            recommendations.append("ìˆ˜ì§‘ì´ ë¹ ë¥´ê²Œ ì™„ë£Œë©ë‹ˆë‹¤. ë” ë§ì€ ì†ŒìŠ¤ë¥¼ í™œì„±í™”í•˜ê±°ë‚˜ ìˆ˜ì§‘ëŸ‰ì„ ëŠ˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # í™œì„± ì†ŒìŠ¤ ìˆ˜ ë¶„ì„
    active_sources = len([s for s in source_performance.values()])
    if active_sources < 3:
        recommendations.append("í™œì„±í™”ëœ ì†ŒìŠ¤ê°€ ì ìŠµë‹ˆë‹¤. ë” ë„“ì€ ë²”ìœ„ì˜ ìœ„í˜‘ ì •ë³´ë¥¼ ìœ„í•´ ì¶”ê°€ ì†ŒìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
    elif active_sources > 7:
        recommendations.append("ë§ì€ ì†ŒìŠ¤ê°€ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ì„ ìœ„í•´ ìš°ì„ ìˆœìœ„ê°€ ë‚®ì€ ì†ŒìŠ¤ë¥¼ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
    if not recommendations:
        recommendations.append("í˜„ì¬ ìˆ˜ì§‘ ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì„±ëŠ¥ì„ ìœ ì§€í•˜ì„¸ìš”.")

    return recommendations
